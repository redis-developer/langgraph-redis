{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Middleware Composition with LangChain Agents\n",
    "\n",
    "This notebook demonstrates how to compose multiple Redis middleware together and use them with LangChain agents using the standard `create_agent` pattern.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Combine multiple middleware**: Stack caching, memory, and routing\n",
    "- **MiddlewareStack**: Compose middleware into a single unit\n",
    "- **Connection sharing**: Share Redis connections with checkpointers\n",
    "- **Factory functions**: Quick setup with `create_caching_stack`\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Redis 8.0+ or Redis Stack\n",
    "- OpenAI API key\n",
    "\n",
    "## Note on Async Usage\n",
    "\n",
    "The Redis middleware uses async methods internally. When using with `create_agent`, you must use `await agent.ainvoke()` rather than `agent.invoke()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and set API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph-checkpoint-redis langchain langchain-openai sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "REDIS_URL = os.environ.get(\"REDIS_URL\", \"redis://redis:6379\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-middleware",
   "metadata": {},
   "source": [
    "## Using Multiple Middleware with create_agent\n",
    "\n",
    "You can pass multiple middleware directly to `create_agent`. They are applied in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-tools",
   "metadata": {},
   "outputs": [],
   "source": "import ast\nimport operator as op\nimport time\n\nfrom langchain_core.tools import tool\n\n# Safe math evaluator - no arbitrary code execution\nSAFE_OPS = {\n    ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul,\n    ast.Div: op.truediv, ast.Pow: op.pow, ast.USub: op.neg,\n}\n\ndef _eval_node(node):\n    if isinstance(node, ast.Constant):\n        return node.value\n    elif isinstance(node, ast.BinOp) and type(node.op) in SAFE_OPS:\n        return SAFE_OPS[type(node.op)](_eval_node(node.left), _eval_node(node.right))\n    elif isinstance(node, ast.UnaryOp) and type(node.op) in SAFE_OPS:\n        return SAFE_OPS[type(node.op)](_eval_node(node.operand))\n    raise ValueError(\"Unsupported expression\")\n\ndef safe_eval(expr: str) -> float:\n    return _eval_node(ast.parse(expr, mode='eval').body)\n\n# Track executions\ntool_calls = {\"search\": 0, \"calculate\": 0}\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    tool_calls[\"search\"] += 1\n    time.sleep(0.5)  # Simulate API call\n    return f\"Search results for '{query}': Found relevant information.\"\n\n\n@tool\ndef calculate(expression: str) -> str:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    tool_calls[\"calculate\"] += 1\n    try:\n        result = safe_eval(expression)\n        return f\"{expression} = {result}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n\ntools = [search, calculate]\nprint(\"Tools defined: search, calculate\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "create-individual-middleware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created coordinated middleware:\n",
      "- Deterministic tools: ['search', 'calculate']\n",
      "- SemanticCacheMiddleware: caches LLM responses\n",
      "- ToolResultCacheMiddleware: caches tool results\n",
      "\n",
      "Both middlewares are aware of which tools are safe to cache!\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "from langgraph.middleware.redis import (\n",
    "    SemanticCacheConfig,\n",
    "    SemanticCacheMiddleware,\n",
    "    ToolCacheConfig,\n",
    "    ToolResultCacheMiddleware,\n",
    ")\n",
    "\n",
    "# Define which tools are deterministic (same input = same output)\n",
    "DETERMINISTIC_TOOLS = [\"search\", \"calculate\"]\n",
    "\n",
    "# Create semantic cache for LLM responses\n",
    "# Note: deterministic_tools should match the cacheable tools\n",
    "semantic_cache = SemanticCacheMiddleware(\n",
    "    SemanticCacheConfig(\n",
    "        redis_url=REDIS_URL,\n",
    "        name=\"composition_llm_cache\",\n",
    "        ttl_seconds=3600,\n",
    "        # When tool results are from these tools, caching is still OK\n",
    "        deterministic_tools=DETERMINISTIC_TOOLS,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create tool cache for tool results\n",
    "# Tools can also use metadata={\"cacheable\": True/False} on the tool itself\n",
    "tool_cache = ToolResultCacheMiddleware(\n",
    "    ToolCacheConfig(\n",
    "        redis_url=REDIS_URL,\n",
    "        name=\"composition_tool_cache\",\n",
    "        # Fallback when tool.metadata[\"cacheable\"] is not set\n",
    "        cacheable_tools=DETERMINISTIC_TOOLS,\n",
    "        ttl_seconds=1800,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Created coordinated middleware:\")\n",
    "print(f\"- Deterministic tools: {DETERMINISTIC_TOOLS}\")\n",
    "print(\"- SemanticCacheMiddleware: caches LLM responses\")\n",
    "print(\"- ToolResultCacheMiddleware: caches tool results\")\n",
    "print(\"\\nBoth middlewares are aware of which tools are safe to cache!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "create-agent-multiple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created with both SemanticCache and ToolCache middleware!\n"
     ]
    }
   ],
   "source": [
    "# Create agent with multiple middleware\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    middleware=[semantic_cache, tool_cache],  # Multiple middleware!\n",
    ")\n",
    "\n",
    "print(\"Agent created with both SemanticCache and ToolCache middleware!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "test-multiple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Search query\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e2abeeca2a4e22a20553275aaf4d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6932c19dec74f87b432a6fa226085db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec02f95ead84da2abe998a866feb0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77769021216349ebbf33c2cedf765486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cfad349fa94f4d8b86d1b04244f313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccd80448a6647b5b034089ec78f7e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/596M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e310e1ed73dc466fb7a605e3e21a0e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447b04a6c5704a02a637878daff26cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91e4e46e5ec445695c916d9e3c91b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201e7f5cdb8348c1899dd33a81ca7ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This vectorizer has no async embed method. Falling back to sync.\n",
      "This vectorizer has no async embed method. Falling back to sync.\n",
      "This vectorizer has no async embed method. Falling back to sync.\n",
      "This vectorizer has no async embed method. Falling back to sync.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I found some relevant information about Python tutorials. Here are a few types of tutorials you migh...\n",
      "Tool calls: {'search': 1, 'calculate': 0}\n",
      "\n",
      "Test 2: Similar search query (should hit cache)\n",
      "==================================================\n",
      "Response: I found some relevant information about Python tutorials. Here are a few types of tutorials you migh...\n",
      "Tool calls: {'search': 1, 'calculate': 0}\n",
      "Note: tool_calls should not increase if cache hit!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Reset counters\n",
    "tool_calls = {\"search\": 0, \"calculate\": 0}\n",
    "\n",
    "print(\"Test 1: Search query\")\n",
    "print(\"=\" * 50)\n",
    "result1 = await agent.ainvoke({\"messages\": [HumanMessage(content=\"Search for Python tutorials\")]})\n",
    "print(f\"Response: {result1['messages'][-1].content[:100]}...\")\n",
    "print(f\"Tool calls: {tool_calls}\")\n",
    "\n",
    "print(\"\\nTest 2: Similar search query (should hit cache)\")\n",
    "print(\"=\" * 50)\n",
    "result2 = await agent.ainvoke({\"messages\": [HumanMessage(content=\"Find Python tutorials online\")]})\n",
    "print(f\"Response: {result2['messages'][-1].content[:100]}...\")\n",
    "print(f\"Tool calls: {tool_calls}\")\n",
    "print(\"Note: tool_calls should not increase if cache hit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middleware-stack",
   "metadata": {},
   "source": [
    "## Using MiddlewareStack\n",
    "\n",
    "The `MiddlewareStack` class lets you compose multiple middleware into a single unit that can be passed to `create_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "create-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiddlewareStack created with:\n",
      "- SemanticCacheMiddleware\n",
      "- ConversationMemoryMiddleware\n"
     ]
    }
   ],
   "source": [
    "from langgraph.middleware.redis import (\n",
    "    ConversationMemoryConfig,\n",
    "    ConversationMemoryMiddleware,\n",
    "    MiddlewareStack,\n",
    ")\n",
    "\n",
    "# Create a stack with cache + memory\n",
    "stack = MiddlewareStack(\n",
    "    [\n",
    "        SemanticCacheMiddleware(\n",
    "            SemanticCacheConfig(\n",
    "                redis_url=REDIS_URL,\n",
    "                name=\"stack_llm_cache\",\n",
    "                ttl_seconds=3600,\n",
    "            )\n",
    "        ),\n",
    "        ConversationMemoryMiddleware(\n",
    "            ConversationMemoryConfig(\n",
    "                redis_url=REDIS_URL,\n",
    "                name=\"stack_memory\",\n",
    "                session_tag=\"stack_demo\",\n",
    "                top_k=3,\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"MiddlewareStack created with:\")\n",
    "print(\"- SemanticCacheMiddleware\")\n",
    "print(\"- ConversationMemoryMiddleware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "use-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created with MiddlewareStack!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This vectorizer has no async embed method. Falling back to sync.\n",
      "This vectorizer has no async embed method. Falling back to sync.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Hello! It sounds like you're working on something interesting. How can I assist you with your testing?\n"
     ]
    }
   ],
   "source": [
    "# Create agent with the stack (stack is also an AgentMiddleware!)\n",
    "agent_with_stack = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    middleware=[stack],  # Pass the stack as a single middleware\n",
    ")\n",
    "\n",
    "print(\"Agent created with MiddlewareStack!\")\n",
    "\n",
    "# Test it\n",
    "result = await agent_with_stack.ainvoke({\"messages\": [HumanMessage(content=\"Hi, I'm testing the middleware stack!\")]})\n",
    "print(f\"Response: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "factory-functions",
   "metadata": {},
   "source": [
    "## Factory Functions\n",
    "\n",
    "For common patterns, use factory functions like `create_caching_stack` and `from_configs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "create-caching-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created caching stack with create_caching_stack()\n",
      "Number of middleware in stack: 2\n"
     ]
    }
   ],
   "source": [
    "from langgraph.middleware.redis import create_caching_stack\n",
    "\n",
    "# Quick setup for caching both LLM and tool results\n",
    "caching_stack = create_caching_stack(\n",
    "    redis_url=REDIS_URL,\n",
    "    semantic_cache_name=\"factory_llm_cache\",\n",
    "    semantic_cache_ttl=3600,\n",
    "    tool_cache_name=\"factory_tool_cache\",\n",
    "    tool_cache_ttl=1800,\n",
    "    cacheable_tools=[\"search\", \"calculate\"],\n",
    ")\n",
    "\n",
    "print(\"Created caching stack with create_caching_stack()\")\n",
    "print(f\"Number of middleware in stack: {len(caching_stack._middlewares)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "from-configs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created custom stack with from_configs()\n",
      "Number of middleware in stack: 3\n"
     ]
    }
   ],
   "source": [
    "from langgraph.middleware.redis import from_configs\n",
    "\n",
    "# Create stack from config objects\n",
    "custom_stack = from_configs(\n",
    "    configs=[\n",
    "        SemanticCacheConfig(\n",
    "            name=\"custom_llm_cache\",\n",
    "            distance_threshold=0.15,\n",
    "            ttl_seconds=3600,\n",
    "        ),\n",
    "        ToolCacheConfig(\n",
    "            name=\"custom_tool_cache\",\n",
    "            cacheable_tools=[\"search\"],\n",
    "            excluded_tools=[\"calculate\"],\n",
    "            ttl_seconds=600,\n",
    "        ),\n",
    "        ConversationMemoryConfig(\n",
    "            name=\"custom_memory\",\n",
    "            session_tag=\"custom_session\",\n",
    "            top_k=5,\n",
    "        ),\n",
    "    ],\n",
    "    redis_url=REDIS_URL,\n",
    ")\n",
    "\n",
    "print(\"Created custom stack with from_configs()\")\n",
    "print(f\"Number of middleware in stack: {len(custom_stack._middlewares)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connection-sharing",
   "metadata": {},
   "source": [
    "## Connection Sharing with Checkpointer\n",
    "\n",
    "Use `IntegratedRedisMiddleware` to share Redis connections with checkpointers for production deployments.\n",
    "\n",
    "**Note**: When using async agent methods (`ainvoke`), you must use `AsyncRedisSaver` instead of the sync `RedisSaver`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "integrated-middleware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created IntegratedRedisMiddleware from AsyncRedisSaver!\n",
      "Number of middleware: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This vectorizer has no async embed method. Falling back to sync.\n",
      "This vectorizer has no async embed method. Falling back to sync.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.redis.aio import AsyncRedisSaver\n",
    "from langgraph.middleware.redis import IntegratedRedisMiddleware\n",
    "\n",
    "# Use AsyncRedisSaver for async operations\n",
    "async_checkpointer = AsyncRedisSaver(redis_url=REDIS_URL)\n",
    "await async_checkpointer.asetup()\n",
    "\n",
    "# Create middleware stack that shares connection\n",
    "integrated_stack = IntegratedRedisMiddleware.from_saver(\n",
    "    async_checkpointer,\n",
    "    configs=[\n",
    "        SemanticCacheConfig(name=\"integrated_cache\", ttl_seconds=3600),\n",
    "        ToolCacheConfig(name=\"integrated_tools\", ttl_seconds=1800),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Created IntegratedRedisMiddleware from AsyncRedisSaver!\")\n",
    "print(f\"Number of middleware: {len(integrated_stack._middlewares)}\")\n",
    "\n",
    "# Create agent with both checkpointer and middleware\n",
    "integrated_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    checkpointer=async_checkpointer,\n",
    "    middleware=[integrated_stack],\n",
    ")\n",
    "\n",
    "# Test it\n",
    "result = await integrated_agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hello!\")]}, config={\"configurable\": {\"thread_id\": \"integrated-test\"}}\n",
    ")\n",
    "print(f\"\\nResponse: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Multiple middleware**: Pass a list to `create_agent(middleware=[...])`\n",
    "- **MiddlewareStack**: Compose middleware into a single unit\n",
    "- **create_caching_stack()**: Quick setup for LLM + tool caching\n",
    "- **from_configs()**: Create stack from config objects\n",
    "- **IntegratedRedisMiddleware**: Share connections with checkpointers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "close",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All middleware closed.\n",
      "Demo complete!\n"
     ]
    }
   ],
   "source": [
    "# Close all middleware\n",
    "await semantic_cache.aclose()\n",
    "await tool_cache.aclose()\n",
    "await stack.aclose()\n",
    "await caching_stack.aclose()\n",
    "await custom_stack.aclose()\n",
    "\n",
    "# Close the async checkpointer\n",
    "try:\n",
    "    await async_checkpointer.aclose()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"All middleware closed.\")\n",
    "print(\"Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}