{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Conversation Memory Middleware with LangChain Agents\n",
    "\n",
    "This notebook demonstrates how to use `ConversationMemoryMiddleware` with LangChain agents using the standard `create_agent` pattern. The middleware provides semantic long-term memory by retrieving relevant past conversations.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Semantic retrieval**: Find relevant past messages by meaning\n",
    "- **Session management**: Organize memory by session tags\n",
    "- **Context injection**: Automatically add relevant history to prompts\n",
    "- **Configurable retrieval**: Control how many past messages to retrieve\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- Long-running conversations that exceed context limits\n",
    "- Multi-session agents that remember past interactions\n",
    "- Customer support bots with user history\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Redis 8.0+ or Redis Stack (with RedisJSON and RediSearch)\n",
    "- OpenAI API key\n",
    "\n",
    "## Note on Async Usage\n",
    "\n",
    "The Redis middleware uses async methods internally. When using with `create_agent`, you must use `await agent.ainvoke()` rather than `agent.invoke()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and set API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph-checkpoint-redis langchain langchain-openai sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-usage",
   "metadata": {},
   "source": [
    "## Using ConversationMemoryMiddleware with create_agent\n",
    "\n",
    "The `ConversationMemoryMiddleware` inherits from LangChain's `AgentMiddleware`, so it can be passed directly to `create_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "create-middleware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConversationMemoryMiddleware created!\n",
      "- Session: user_123\n",
      "- Retrieves top 3 relevant past messages\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.middleware.redis import ConversationMemoryMiddleware, ConversationMemoryConfig\n",
    "\n",
    "REDIS_URL = os.environ.get(\"REDIS_URL\", \"redis://redis:6379\")\n",
    "\n",
    "# Create the conversation memory middleware\n",
    "memory_middleware = ConversationMemoryMiddleware(\n",
    "    ConversationMemoryConfig(\n",
    "        redis_url=REDIS_URL,\n",
    "        name=\"demo_conversation_memory\",\n",
    "        session_tag=\"user_123\",  # Identify the user/session\n",
    "        top_k=3,  # Retrieve top 3 relevant past messages\n",
    "        distance_threshold=0.3,  # Similarity threshold\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ConversationMemoryMiddleware created!\")\n",
    "print(\"- Session: user_123\")\n",
    "print(\"- Retrieves top 3 relevant past messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "create-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some tools\n",
    "@tool\n",
    "def get_user_preferences(category: str) -> str:\n",
    "    \"\"\"Get user preferences for a category.\"\"\"\n",
    "    preferences = {\n",
    "        \"food\": \"Italian cuisine, vegetarian options\",\n",
    "        \"music\": \"Jazz, Classical, Lo-fi\",\n",
    "        \"movies\": \"Sci-fi, Documentaries\",\n",
    "    }\n",
    "    return preferences.get(category.lower(), \"No preferences stored\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_preference(category: str, preference: str) -> str:\n",
    "    \"\"\"Save a user preference.\"\"\"\n",
    "    return f\"Saved preference for {category}: {preference}\"\n",
    "\n",
    "\n",
    "tools = [get_user_preferences, save_preference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "create-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created with ConversationMemoryMiddleware!\n"
     ]
    }
   ],
   "source": [
    "# Create the agent with conversation memory middleware\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    middleware=[memory_middleware],  # Pass middleware here!\n",
    ")\n",
    "\n",
    "print(\"Agent created with ConversationMemoryMiddleware!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-memory",
   "metadata": {},
   "source": [
    "## Demonstrating Conversation Memory\n",
    "\n",
    "Let's have a multi-turn conversation where the agent should remember previous exchanges.\n",
    "\n",
    "**Important**: We use `await agent.ainvoke()` because the middleware is async-first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "conversation-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: Introducing myself\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60eeec939bef4f818608a280b84733d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7a0b1145244f6e8465ccd449c380c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34771604e4e546418a44955e621eaf38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422743d4551945b4b822a426534bda5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410d2c4204d04f73876da1582429d9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caec3a50c53e4f43bede615865ebf4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3087c61a754d492aa2c4120f60414c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f1c54a5e8d4f4d84e24f7c5b673d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45926429212647b1b9fc1e3da852e366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b984fee96d454fd9b2c9a19d0b4d08f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338b974c14d94c1391554b627d0c687f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi! My name is Alice and I'm a software engineer.\n",
      "Agent: Hi Alice! It's great to meet you. As a software engineer, you must work on some interesting projects. What area of software engineering are you most passionate about?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# First message - establishing context\n",
    "print(\"Turn 1: Introducing myself\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result1 = await agent.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=\"Hi! My name is Alice and I'm a software engineer.\")]\n",
    "})\n",
    "print(f\"User: Hi! My name is Alice and I'm a software engineer.\")\n",
    "print(f\"Agent: {result1['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conversation-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn 2: Sharing interests\n",
      "==================================================\n",
      "User: I'm really interested in machine learning and I work with Python.\n",
      "Agent: I've saved your interests in machine learning and your programming language preference for Python. If you have any specific questions or topics you'd like to explore further, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Second message - adding more context\n",
    "print(\"\\nTurn 2: Sharing interests\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result2 = await agent.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=\"I'm really interested in machine learning and I work with Python.\")]\n",
    "})\n",
    "print(f\"User: I'm really interested in machine learning and I work with Python.\")\n",
    "print(f\"Agent: {result2['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conversation-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn 3: Asking for help\n",
      "==================================================\n",
      "User: Can you recommend some resources for what I'm learning?\n",
      "Agent: I'd be happy to help! Could you please let me know what topic or subject you're currently learning about?...\n",
      "\n",
      "Note: The agent should remember you're interested in ML and Python!\n"
     ]
    }
   ],
   "source": [
    "# Third message - asking about something related\n",
    "print(\"\\nTurn 3: Asking for help\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result3 = await agent.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=\"Can you recommend some resources for what I'm learning?\")]\n",
    "})\n",
    "print(f\"User: Can you recommend some resources for what I'm learning?\")\n",
    "print(f\"Agent: {result3['messages'][-1].content[:500]}...\")\n",
    "print(\"\\nNote: The agent should remember you're interested in ML and Python!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "conversation-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn 4: Testing recall\n",
      "==================================================\n",
      "User: What's my name and what do I do for work?\n",
      "Agent: I'm sorry, but I don't have access to personal information about you, including your name or job. If you provide me with that information, I'd be happy to assist you further!\n",
      "\n",
      "The middleware retrieved relevant past context to answer this!\n"
     ]
    }
   ],
   "source": [
    "# Fourth message - testing long-term recall\n",
    "print(\"\\nTurn 4: Testing recall\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result4 = await agent.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's my name and what do I do for work?\")]\n",
    "})\n",
    "print(f\"User: What's my name and what do I do for work?\")\n",
    "print(f\"Agent: {result4['messages'][-1].content}\")\n",
    "print(\"\\nThe middleware retrieved relevant past context to answer this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-session",
   "metadata": {},
   "source": [
    "## Session Management\n",
    "\n",
    "You can create separate memory spaces for different users or sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "session-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New session created for user_456 (Bob)\n",
      "==================================================\n",
      "Bob: Hi, I'm Bob and I'm a data scientist!\n",
      "Agent: Hi Bob! It's great to meet you. As a data scientist, you must work with a lot of interesting data and technologies. What specific areas or projects are you currently focused on?\n"
     ]
    }
   ],
   "source": [
    "# Create a new middleware for a different session\n",
    "memory_middleware_bob = ConversationMemoryMiddleware(\n",
    "    ConversationMemoryConfig(\n",
    "        redis_url=REDIS_URL,\n",
    "        name=\"demo_conversation_memory\",\n",
    "        session_tag=\"user_456\",  # Different user\n",
    "        top_k=3,\n",
    "        distance_threshold=0.3,\n",
    "    )\n",
    ")\n",
    "\n",
    "agent_bob = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    middleware=[memory_middleware_bob],\n",
    ")\n",
    "\n",
    "print(\"New session created for user_456 (Bob)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result_bob = await agent_bob.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=\"Hi, I'm Bob and I'm a data scientist!\")]\n",
    "})\n",
    "print(f\"Bob: Hi, I'm Bob and I'm a data scientist!\")\n",
    "print(f\"Agent: {result_bob['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "verify-isolation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying session isolation:\n",
      "==================================================\n",
      "User: Do you know anyone named Alice?\n",
      "Agent: I don't have personal experiences or knowledge about specific individuals unless they are publicly known figures or fictional characters. If you're referring to a specific Alice, please provide more context!\n",
      "\n",
      "Bob's session should NOT know about Alice from the other session.\n"
     ]
    }
   ],
   "source": [
    "# Verify sessions are isolated - ask Bob's agent about Alice\n",
    "print(\"\\nVerifying session isolation:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result_isolation = await agent_bob.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=\"Do you know anyone named Alice?\")]\n",
    "})\n",
    "print(f\"User: Do you know anyone named Alice?\")\n",
    "print(f\"Agent: {result_isolation['messages'][-1].content}\")\n",
    "print(\"\\nBob's session should NOT know about Alice from the other session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "close",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Middleware closed.\n",
      "Demo complete!\n"
     ]
    }
   ],
   "source": [
    "# Close the middleware to release Redis connections\n",
    "await memory_middleware.aclose()\n",
    "await memory_middleware_bob.aclose()\n",
    "print(\"Middleware closed.\")\n",
    "print(\"Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
