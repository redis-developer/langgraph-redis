{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Semantic Cache Middleware with LangChain Agents\n",
    "\n",
    "This notebook demonstrates how to use `SemanticCacheMiddleware` with LangChain agents using the standard `create_agent` pattern.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Semantic matching**: Cache similar prompts, not just exact matches\n",
    "- **Cost reduction**: Avoid redundant LLM calls for similar queries\n",
    "- **Latency improvement**: Instant responses for cached queries\n",
    "- **Tool-aware caching**: Smart handling of tool-calling workflows\n",
    "\n",
    "## Tool-Aware Caching\n",
    "\n",
    "When the LLM uses tools, caching behavior depends on whether tools are deterministic:\n",
    "\n",
    "- **Deterministic tools** (e.g., calculator): Same input always produces same output. Safe to cache.\n",
    "- **Non-deterministic tools** (e.g., stock prices): Output changes over time. Don't cache.\n",
    "\n",
    "Configure this via `deterministic_tools` in `SemanticCacheConfig`:\n",
    "```python\n",
    "SemanticCacheConfig(\n",
    "    deterministic_tools=[\"calculate\", \"convert_units\"],  # Safe to cache after these\n",
    ")\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Redis 8.0+ or Redis Stack (with RedisJSON and RediSearch)\n",
    "- OpenAI API key\n",
    "\n",
    "## Note on Async Usage\n",
    "\n",
    "The Redis middleware uses async methods internally. When using with `create_agent`, you must use `await agent.ainvoke()` rather than `agent.invoke()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and set API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph-checkpoint-redis langchain langchain-openai sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-usage",
   "metadata": {},
   "source": [
    "## Using SemanticCacheMiddleware with create_agent\n",
    "\n",
    "The `SemanticCacheMiddleware` inherits from LangChain's `AgentMiddleware`, so it can be passed directly to `create_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "create-middleware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticCacheMiddleware created!\n",
      "- distance_threshold: 0.15 (semantic matching)\n",
      "- cache_final_only: True (don't cache tool-calling responses)\n",
      "- deterministic_tools: ['calculate'] (safe to cache after these tools)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "from langgraph.middleware.redis import SemanticCacheMiddleware, SemanticCacheConfig\n",
    "\n",
    "REDIS_URL = os.environ.get(\"REDIS_URL\", \"redis://redis:6379\")\n",
    "\n",
    "# Create the semantic cache middleware\n",
    "cache_middleware = SemanticCacheMiddleware(\n",
    "    SemanticCacheConfig(\n",
    "        redis_url=REDIS_URL,\n",
    "        name=\"demo_semantic_cache\",\n",
    "        distance_threshold=0.15,  # Lower = stricter matching\n",
    "        ttl_seconds=3600,  # Cache entries expire after 1 hour\n",
    "        cache_final_only=True,  # Only cache final responses (not tool calls)\n",
    "        # deterministic_tools: List tools whose results are deterministic.\n",
    "        # When set, cache can be used even after these tools execute.\n",
    "        # If None (default), cache is skipped when any tool results are present.\n",
    "        deterministic_tools=[\"calculate\"],  # Calculator is deterministic\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"SemanticCacheMiddleware created!\")\n",
    "print(\"- distance_threshold: 0.15 (semantic matching)\")\n",
    "print(\"- cache_final_only: True (don't cache tool-calling responses)\")\n",
    "print(\"- deterministic_tools: ['calculate'] (safe to cache after these tools)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-tools",
   "metadata": {},
   "outputs": [],
   "source": "import ast\nimport operator as op\n\n# Safe math evaluator - no arbitrary code execution\nSAFE_OPS = {\n    ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul,\n    ast.Div: op.truediv, ast.Pow: op.pow, ast.USub: op.neg,\n}\n\ndef _eval_node(node):\n    if isinstance(node, ast.Constant):\n        return node.value\n    elif isinstance(node, ast.BinOp) and type(node.op) in SAFE_OPS:\n        return SAFE_OPS[type(node.op)](_eval_node(node.left), _eval_node(node.right))\n    elif isinstance(node, ast.UnaryOp) and type(node.op) in SAFE_OPS:\n        return SAFE_OPS[type(node.op)](_eval_node(node.operand))\n    raise ValueError(\"Unsupported expression\")\n\ndef safe_eval(expr: str) -> float:\n    return _eval_node(ast.parse(expr, mode='eval').body)\n\n\n# Define some tools for the agent\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # Simulated weather data\n    weather_data = {\n        \"new york\": \"72°F, Partly Cloudy\",\n        \"san francisco\": \"65°F, Foggy\",\n        \"london\": \"58°F, Rainy\",\n        \"tokyo\": \"80°F, Sunny\",\n    }\n    location_lower = location.lower()\n    for city, weather in weather_data.items():\n        if city in location_lower:\n            return f\"Weather in {location}: {weather}\"\n    return f\"Weather data not available for {location}\"\n\n\n@tool\ndef calculate(expression: str) -> str:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    try:\n        result = safe_eval(expression)\n        return f\"{expression} = {result}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n\ntools = [get_weather, calculate]"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "create-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created with SemanticCacheMiddleware!\n"
     ]
    }
   ],
   "source": [
    "# Create the agent with semantic cache middleware\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    middleware=[cache_middleware],  # Pass middleware here!\n",
    ")\n",
    "\n",
    "print(\"Agent created with SemanticCacheMiddleware!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-caching",
   "metadata": {},
   "source": [
    "## Demonstrating Cache Behavior\n",
    "\n",
    "Let's make some queries and observe how the cache works. The first query will hit the LLM, while semantically similar queries should hit the cache.\n",
    "\n",
    "**Important**: We use `await agent.ainvoke()` because the middleware is async-first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "first-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: 'What is the capital of France?'\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This vectorizer has no async embed method. Falling back to sync.\n",
      "This vectorizer has no async embed method. Falling back to sync.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris....\n",
      "Time: 4.44s (cache miss - LLM call)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# First query - will be a cache miss (hits the LLM)\n",
    "print(\"Query 1: 'What is the capital of France?'\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start = time.time()\n",
    "result1 = await agent.ainvoke({\"messages\": [HumanMessage(content=\"What is the capital of France?\")]})\n",
    "elapsed1 = time.time() - start\n",
    "\n",
    "print(f\"Response: {result1['messages'][-1].content[:200]}...\")\n",
    "print(f\"Time: {elapsed1:.2f}s (cache miss - LLM call)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "second-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This vectorizer has no async embed method. Falling back to sync.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 2: 'Tell me France's capital city'\n",
      "==================================================\n",
      "Response: The capital of France is Paris....\n",
      "Time: 0.06s (expected: cache hit - much faster!)\n"
     ]
    }
   ],
   "source": [
    "# Second query - semantically similar, should hit cache\n",
    "print(\"\\nQuery 2: 'Tell me France's capital city'\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start = time.time()\n",
    "result2 = await agent.ainvoke({\"messages\": [HumanMessage(content=\"Tell me France's capital city\")]})\n",
    "elapsed2 = time.time() - start\n",
    "\n",
    "print(f\"Response: {result2['messages'][-1].content[:200]}...\")\n",
    "print(f\"Time: {elapsed2:.2f}s (expected: cache hit - much faster!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "third-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This vectorizer has no async embed method. Falling back to sync.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 3: 'What is the capital of Germany?'\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This vectorizer has no async embed method. Falling back to sync.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of Germany is Berlin....\n",
      "Time: 0.74s (cache miss - different topic)\n"
     ]
    }
   ],
   "source": [
    "# Third query - different topic, should be cache miss\n",
    "print(\"\\nQuery 3: 'What is the capital of Germany?'\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start = time.time()\n",
    "result3 = await agent.ainvoke({\"messages\": [HumanMessage(content=\"What is the capital of Germany?\")]})\n",
    "elapsed3 = time.time() - start\n",
    "\n",
    "print(f\"Response: {result3['messages'][-1].content[:200]}...\")\n",
    "print(f\"Time: {elapsed3:.2f}s (cache miss - different topic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "Query 1 (France capital, miss): 4.44s\n",
      "Query 2 (France capital, hit):  0.06s\n",
      "Query 3 (Germany capital, miss): 0.74s\n",
      "\n",
      " Cache hit was significantly faster!\n",
      "  Speedup: 71.0x\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Query 1 (France capital, miss): {elapsed1:.2f}s\")\n",
    "print(f\"Query 2 (France capital, hit):  {elapsed2:.2f}s\")\n",
    "print(f\"Query 3 (Germany capital, miss): {elapsed3:.2f}s\")\n",
    "\n",
    "if elapsed2 < elapsed1 * 0.5:\n",
    "    print(\"\\n Cache hit was significantly faster!\")\n",
    "    print(f\"  Speedup: {elapsed1/elapsed2:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool-queries",
   "metadata": {},
   "source": [
    "## Tool Queries and Cache Behavior\n",
    "\n",
    "By default, `cache_final_only=True` means only final responses (without tool calls) are cached. This prevents caching intermediate tool-calling responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tool-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This vectorizer has no async embed method. Falling back to sync.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query with tool: 'What's the weather in Tokyo?'\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This vectorizer has no async embed method. Falling back to sync.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The weather in Tokyo is currently 80°F and sunny.\n",
      "Time: 1.48s\n",
      "\n",
      "Note: The final response (after tool execution) is cached, not the tool-calling step.\n"
     ]
    }
   ],
   "source": [
    "# Query that requires a tool call\n",
    "print(\"Query with tool: 'What's the weather in Tokyo?'\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start = time.time()\n",
    "result = await agent.ainvoke({\"messages\": [HumanMessage(content=\"What's the weather in Tokyo?\")]})\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Response: {result['messages'][-1].content}\")\n",
    "print(f\"Time: {elapsed:.2f}s\")\n",
    "print(\"\\nNote: The final response (after tool execution) is cached, not the tool-calling step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "close",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Middleware closed.\n",
      "Demo complete!\n"
     ]
    }
   ],
   "source": [
    "# Close the middleware to release Redis connection\n",
    "await cache_middleware.aclose()\n",
    "print(\"Middleware closed.\")\n",
    "print(\"Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}