{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992c4695-ec4f-428d-bd05-fb3b5fbd70f4",
   "metadata": {},
   "source": [
    "# How to manage conversation history in a ReAct Agent with Redis\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - [Prebuilt create_react_agent](../create-react-agent)\n",
    "    - [Persistence](../../concepts/persistence)\n",
    "    - [Short-term Memory](../../concepts/memory/#short-term-memory)\n",
    "    - [Trimming Messages](https://python.langchain.com/docs/how_to/trim_messages/)\n",
    "\n",
    "Message history can grow quickly and exceed LLM context window size, whether you're building chatbots with many conversation turns or agentic systems with numerous tool calls. There are several strategies for managing the message history:\n",
    "\n",
    "* [message trimming](#keep-the-original-message-history-unmodified) - remove first or last N messages in the history\n",
    "* [summarization](#summarizing-message-history) - summarize earlier messages in the history and replace them with a summary\n",
    "* custom strategies (e.g., message filtering, etc.)\n",
    "\n",
    "To manage message history in `create_react_agent`, you need to define a `pre_model_hook` function or [runnable](https://python.langchain.com/docs/concepts/runnables/) that takes graph state an returns a state update:\n",
    "\n",
    "\n",
    "* Trimming example:\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langchain_core.messages.utils import (\n",
    "        # highlight-next-line\n",
    "        trim_messages, \n",
    "        # highlight-next-line\n",
    "        count_tokens_approximately\n",
    "    # highlight-next-line\n",
    "    )\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    from langgraph.checkpoint.redis import RedisSaver\n",
    "    \n",
    "    # This function will be called every time before the node that calls LLM\n",
    "    def pre_model_hook(state):\n",
    "        trimmed_messages = trim_messages(\n",
    "            state[\"messages\"],\n",
    "            strategy=\"last\",\n",
    "            token_counter=count_tokens_approximately,\n",
    "            max_tokens=384,\n",
    "            start_on=\"human\",\n",
    "            end_on=(\"human\", \"tool\"),\n",
    "        )\n",
    "        # You can return updated messages either under `llm_input_messages` or \n",
    "        # `messages` key (see the note below)\n",
    "        # highlight-next-line\n",
    "        return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "    # Set up Redis connection for checkpointer\n",
    "    REDIS_URI = \"redis://redis:6379\"\n",
    "    checkpointer = None\n",
    "    with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "        cp.setup()\n",
    "        checkpointer = cp\n",
    "        \n",
    "    agent = create_react_agent(\n",
    "        model,\n",
    "        tools,\n",
    "        # highlight-next-line\n",
    "        pre_model_hook=pre_model_hook,\n",
    "        checkpointer=checkpointer,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "* Summarization example:\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langmem.short_term import SummarizationNode\n",
    "    from langchain_core.messages.utils import count_tokens_approximately\n",
    "    from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "    from langgraph.checkpoint.redis import RedisSaver\n",
    "    from typing import Any\n",
    "    \n",
    "    model = ChatOpenAI(model=\"gpt-4o\")\n",
    "    \n",
    "    summarization_node = SummarizationNode(\n",
    "        token_counter=count_tokens_approximately,\n",
    "        model=model,\n",
    "        max_tokens=384,\n",
    "        max_summary_tokens=128,\n",
    "        output_messages_key=\"llm_input_messages\",\n",
    "    )\n",
    "\n",
    "    class State(AgentState):\n",
    "        # NOTE: we're adding this key to keep track of previous summary information\n",
    "        # to make sure we're not summarizing on every LLM call\n",
    "        # highlight-next-line\n",
    "        context: dict[str, Any]\n",
    "    \n",
    "    # Set up Redis connection for checkpointer\n",
    "    REDIS_URI = \"redis://redis:6379\"\n",
    "    checkpointer = None\n",
    "    with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "        cp.setup()\n",
    "        checkpointer = cp\n",
    "    \n",
    "    graph = create_react_agent(\n",
    "        model,\n",
    "        tools,\n",
    "        # highlight-next-line\n",
    "        pre_model_hook=summarization_node,\n",
    "        # highlight-next-line\n",
    "        state_schema=State,\n",
    "        checkpointer=checkpointer,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "!!! Important\n",
    "    \n",
    "    * To **keep the original message history unmodified** in the graph state and pass the updated history **only as the input to the LLM**, return updated messages under `llm_input_messages` key\n",
    "    * To **overwrite the original message history** in the graph state with the updated history, return updated messages under `messages` key\n",
    "    \n",
    "    To overwrite the `messages` key, you need to do the following:\n",
    "\n",
    "    ```python\n",
    "    from langchain_core.messages import RemoveMessage\n",
    "    from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "    def pre_model_hook(state):\n",
    "        updated_messages = ...\n",
    "        return {\n",
    "            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES), *updated_messages]\n",
    "            ...\n",
    "        }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be3889f-3c17-4fa1-bd2b-84114a2c7247",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a213e11a-5c62-4ddb-a707-490d91add383",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-openai \"httpx>=0.24.0,<1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a1885c-04ab-4750-aefa-105891fddf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        value = getpass.getpass(f\"{var}: \")\n",
    "        if value.strip():\n",
    "            os.environ[var] = value\n",
    "\n",
    "\n",
    "# Try to set OpenAI API key\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a00ce9",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0f089-070c-4cd4-87e0-6c51f2477b82",
   "metadata": {},
   "source": [
    "## Keep the original message history unmodified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6cbd3a-8632-47ae-9ec5-eec8d7b05cae",
   "metadata": {},
   "source": [
    "Let's build a ReAct agent with a step that manages the conversation history: when the length of the history exceeds a specified number of tokens, we will call [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) utility that that will reduce the history while satisfying LLM provider constraints.\n",
    "\n",
    "There are two ways that the updated message history can be applied inside ReAct agent:\n",
    "\n",
    "  * [**Keep the original message history unmodified**](#keep-the-original-message-history-unmodified) in the graph state and pass the updated history **only as the input to the LLM**\n",
    "  * [**Overwrite the original message history**](#overwrite-the-original-message-history) in the graph state with the updated history\n",
    "\n",
    "Let's start by implementing the first one. We'll need to first define model and tools for our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaad19ee-e174-4c6c-b2b8-3530d7acea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if any([city in location.lower() for city in [\"nyc\", \"new york city\"]]):\n",
    "        return \"It might be cloudy in nyc, with a chance of rain and temperatures up to 80 degrees.\"\n",
    "    elif any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n",
    "        return \"It's always sunny in sf\"\n",
    "    else:\n",
    "        return f\"I am not sure what the weather is in {location}\"\n",
    "\n",
    "\n",
    "tools = [get_weather]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52402333-61ab-47d3-8549-6a70f6f1cf36",
   "metadata": {},
   "source": [
    "Now let's implement `pre_model_hook` — a function that will be added as a new node and called every time **before** the node that calls the LLM (the `agent` node).\n",
    "\n",
    "Our implementation will wrap the `trim_messages` call and return the trimmed messages under `llm_input_messages`. This will **keep the original message history unmodified** in the graph state and pass the updated history **only as the input to the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b507eb58-6e02-4ac6-b48b-ea4defdc11f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:25:53 langgraph.checkpoint.redis INFO   Redis client is a standalone client\n",
      "17:25:53 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "17:25:53 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "17:25:53 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_387/1628370867.py:37: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  graph = create_react_agent(\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "\n",
    "# highlight-next-line\n",
    "from langchain_core.messages.utils import (\n",
    "    # highlight-next-line\n",
    "    trim_messages,\n",
    "    # highlight-next-line\n",
    "    count_tokens_approximately,\n",
    "    # highlight-next-line\n",
    ")\n",
    "\n",
    "\n",
    "# This function will be added as a new node in ReAct agent graph\n",
    "# that will run every time before the node that calls the LLM.\n",
    "# The messages returned by this function will be the input to the LLM.\n",
    "def pre_model_hook(state):\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=384,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    # highlight-next-line\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "\n",
    "# Set up Redis connection for checkpointer\n",
    "REDIS_URI = \"redis://redis:6379\"\n",
    "checkpointer = None\n",
    "with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "    cp.setup()\n",
    "    checkpointer = cp\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    # highlight-next-line\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8182ab45-86b3-4d6f-b75e-58862a14fa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAFcCAIAAAAlFOfAAAAQAElEQVR4nOydB3zTxhfHT7IdZ+9NSEICSSBsCFCgbEqh7FE2lBmg7FU2ZRUKlPIvUEbLaCmrjDIKLaWMtuxV9mogg0wggSTOtqX/s5U4TuKE2Ei2pNz3E4ysk2RZ+vndvXene1KaphEGY26kCIPhAViIGF6AhYjhBViIGF6AhYjhBViIGF5QEYWYFJ1z91Kq4rUqK12pUlGqXESQiKbyXzXQBEmo/6MQBLcIzSrtNsx67RrNEkIFQTCdgyBSiihl8fWEBNEq+I+GDyk8p6Jv1RvThccEZJaEzIK0tJF4B1o3aOuARAdRceKIsU9y/vnlZcqLbIqiSQlh5yiDV1KClDlUgcgImtJcDQIRBCPEwovDlBYRooSgVeoNYFuaLrIZsyyREiolXbC+yF66u6BihyiqZgYLS4lKifJyVDlZlFJJS+VkpSqWnUd5IbFQIYSY9Fx1/LvnWRlKJ3fLWs3sazW3R4KGQmf3v3p2X5GdoXL3lfeZ5IOEj/iFuG9NXEpCduUQ284jPJC4SI7P+3VrfGa6qnUfj5AwGyRkRC7ELXMjZVJi2CJ/JF4eX1ec3f/CJ8i68whPJFjELMSt8yO9A206fuKOKgBb50c1bO9Up4VQ/RjRCnHTZ08Da9u3H+iGKgzfz4t08ZL3+NQbCRASiZFtC6N8g20rlAqBkUurvIrPPv/LKyRARCjEo1sSwdB3Gi4216Q8jFgccPtCKhIgohOiCj1/nDH88yqoQgJhUZ+qVlsXRCKhITYh/rjiuXtlK1SB6TbGG4Lej64okKAQlxBplJ6S02eSIFvrLOJVxerSby+RoBCVEI9uTrC0kiICmZJZs2YdOXIEGU779u3j4uIQB3QZ5Z2RpkKCQlRCTHqe4xdq6g6GBw8eIMNJSEh4/fo14gapDFlaS07vEZJRFJUQc7NVDds4I264cOFCeHh48+bNu3fvvnDhwlev1FGShg0bxsfHL1mypFWrVvBWoVBs2rRp6NChzGZff/11dnY2s3vbtm337NkzatQo2OWvv/7q0qULrOzWrdu0adMQBzi4yRIis5BwEI8Qn97JJEnk6CFBHPDo0aNJkyaFhYUdOHBg5syZT548+fzzz5FGnfA6f/78c+fOwcLevXt37NgxePDgtWvXwvanTp3asmULcwSZTPbLL78EBwdv2LChWbNmsAGshDr9q6++Qhzg5WeVma5EwkE84xETI7MkMq6ah7du3bK0tBw+fDhJkp6enjVq1IiIiCi52aBBg8DyVamSHzy6ffv2xYsXJ06ciNTjvAgHB4fp06cjk+Dpb3Pv4hskHMQjxEyFiiS5EmLdunWhkp08eXLjxo1btGhRuXJlqGFLbgZm79KlS1Bxg8lUKtUGydm5sKkA8kWmwsGJpCghdd6Kp2rWXHeuLn1ISMg333zj5ua2bt26Hj16jBs3Dqxdyc2gFOpi2ODw4cPXr18fNmyYbqmFhQUyGVIJMnH44N0QjxCtbKQUlyGLpk2bQlvw2LFj0DpMTU0F68jYPC00TR88eLBv374gRKi+YU16ejoyE+mvc5GgEI8Q3X0s8nIpxA03btyA1h4sgFHs3LkzuLogMgjB6G6Tl5eXlZXl7p4/6iw3N/fvv/9GZiIpKltY91Y8QgxpZEfTEMHhpHaGihic5UOHDkHw7969e+AdgyK9vLzkcjko7/Lly1ARgx/j7+9/9OjR2NjYN2/eLF68GFqWaWlpGRkZJQ8IW8IruNVwNMQBcU8zIZSIhIOo4ogyC/LKbymIA8Adhgp39erV0B0yevRoGxsbaAtKpWpXD1zpa9eugY0Ec/jFF1+Ac927d28IIjZq1Gj8+PHwtl27dhBrLHZAHx8fCCVC0BGalYgD3rzI9fIVUp+7qAbG7lvzPDNNOayiDr3RZf2UCLgONg6CMYqisojt+nsIro+VC05sT5DKCQGpEInsAXsXLwu5FXlkU3y3MfoH4KhUKgg46y0C3wKigMzjzMUICAjYtm0b4oYdGvQW2draQp+h3qLQ0FDooUGlEHU/o25rrro6OUJsz6zE/pd9eGPs+DVVS9ugZHONAW453Hi9RdAW1PrCrJOuQW8RhNChiam3CH4z4C3pLTq1++WzO2nhKwKRoBDhw1O7VsRQKnrwXD9UIdkwLaLnWF+vqiYMnrOBCJ9ZGTjLF7r7rv0hpJ5WttixKMo3yFZwKkRifYovfHnA1T9epb6sWNOD710VK5ESXcIF+Zi9mB+w3zD9afu+nkECn4ujnPywJNrFWy7cyR5EPuXIt9OeegdYdRfmM+flZ9uCKLk1CW0SJFjEPwnTts+jcjJUTT5yrddKhNMKHtoQF/8sK6iewweDhD2bQIWYlu78keS7598QEqJKqM0HgzwIIQ2P0k/Uvcwrf6SkJORY2Ug+meOPhOecFKcCTdT514GXT24rwDoSpHrMmJ2TzNpWKpFRebl6ZsiEBUI9xhHpTgdLSggIDJEks17j6VH5c2xqxuTSzHpYVP9Pq/ckCjZWT0FLq4eKkVKCYmbv1Oyu/SDtYeFTkOau6M7VKZXBXkRGujIzXT3RLZQ6uVm8393VJ0gkD3FXICFquXDk1fP/srLTVXlK9bfXTuqKdCduJWhCfXGK7KhWGF24TcECjfInmM1fT9MaeRJFVqqX1Z+mnqOWGTeZvzuhUbrOYUlCPY+xdkcGqQzi6qSFJengalGtrm1wmC0SFxVRiFwzYcKEAQMGvPfeewhTbnBWAfZRKpXMCDFM+cHXi32wEI0AXy/2wUI0Any92CcvL08mkyGMIWAhsg+2iEaArxf7YCEaAb5e7IOFaAT4erEPCBG3EQ0FC5F9sEU0Any92AcL0Qjw9WIfLEQjwNeLfbAQjQBfL/aBgDYWoqHg68UyNE1TFCWRCGmWBT6AhcgyuF42DnzJWAYL0TjwJWMZPOLBOLAQWQZbROPAl4xlsBCNA18ylsFCNA58yVgGC9E48CVjGeysGAcWIstgi2gc+JKxT2lzuWLKAAuRZaBzLzExEWEMBAuRZaBeLpYaDVMesBBZBgvROLAQWQYL0TiwEFkGC9E4sBBZBgvROLAQWQYL0TiwEFkGC9E4sBBZBgvROLAQWQaEqFLhDKkGI87MU+YFOlewFg0FC5F9cO1sBFiI7IOFaAS4jcg+WIhGgIXIPliIRoCFyD5YiEaAhcg+WIhGgDNPsUbdunUhcMNcT3XOPZKE186dOy9evBhh3gb2mlmjdu3ajP4AUCRBEJ6engMHDkSYcoCFyBpDhgyxsbHRXQM2Mjg4GGHKARYia7Rr1y4oKEj71sXFpX///ghTPrAQ2WTYsGH29vbMckhISK1atRCmfGAhsknz5s1Bf7Dg4OCAW4cGYWav+d4/GQnRGdlZ6mAHQRI0VZiRm0mkrUnwTlBUYcp33S1JWFBneqe1a7Qp6JEmMzxiss9rU8HrHIHJ001TRbKFFyarl6q3pIp+nPbcYEN1EnFKT1Fq2ps7d+7a2drVq1+vsEjz0aQmUz3zIYXnyRSSBEXTSGdz7QaExlbQlJ5d4IiUssjtg1PQZCsvuGLaQ9Haa6FdqT5z9ecW2VLnshQAfpeljUXdls7OngTiDLMJ8eGlzH+OJoLepBZEbpb6exMQ7qA0V1FzMWiC1mSRR8yiep8CGSGSRgVbIibTuzrpO8F8IaTdOD8/fP5hdUvVSeI1O6tvsE7W+vx91Vcfqaj8O0eoL5L2HhQknc9fLNyFWYa7TmnkRmp+TYVfWPPDoAuEWGRH9edpvprurSj8RpozhFISfhmFNZhaoLC+2CgfsuDHROmupBBN5p9wseOTRbaEW6CRRFHJSpBURihzaBtH6eA5vogbzCPE1GTV7pXRDdq6Vm9sjzAC4dRPiWmvcj5Z6Ic4wAxCVCnQlsVPB80NRBihcfz7hNyM3CEL2NeiGZyVAxtjndytEEaAfDTSKzNTFfMwB7GNGYSY9kbpWQULUajI5eSDK28Q25hh0IMyRyWz4ND/wnCKkqYVaewP6TCDEFUUTVEUwggTWomUeezfPjwMDMMLsBAxvAALEcMLsBAxhqHudOUg1mIeIdLYaRYs0NlIc+BqmkeIBH48AVMUXDVjeIEZhKge9ULgulmokCQt4UA1ZhCiZpQRrpuFCkUTNAczTJnDIpLYHAoZGlEcmBFzWEQKm0NMcfAzK5yw9n8rho34uOxtnj2LaN224d27t8rebNkX8yZMGoHYo1uPtj/u/B7xDOw1Y3gBFiLGcDho5JuhalY/emfIN3ny3yOowv7+58yIUf1goffHH274dg1TdPDQ3l59Opy/cK5t+0brNqxGmiy1m7d8A9XiR11afDZ74uXL5996/MjIp3DY+/fvTJoyChb6D+hy5OiBmJioocN6w2E/nTDs0eMH2o2hUhs4uHuHjk0HD+351Zpl2vFsmZmZc+dP7dT5fdj+jz+O6x4/JSV56bK5/QZ07t6z3bLl858/j0YGIpPKbt260advx/YdmowdN+TBw3tvPZ+yi7TAYeGY5blKReCgkW8GIVLqAIAB20s1Yaufftq6dMmak79d/HTctCNH9x8/cRhWWlhYZGZmHD16YPasxT26qdtk36xbeeDg7h7d++7edaxli7YLF8386+/TZR+fyfO9fsPqoUNGn/nzWmjNOt99vw4aeZ/N/Bw+Tm4hh2MyW27fsenwkZ/Hhk8+sP/kiOHjzv11av+BXUzR6q+WxMbGrF61ccmi1ZFRTy9fyb+1KpVqyrTwW7dvTJk8Z9v3+5wcncd9OjQuPhYZQtKLxKPHDsyZvWTF8m9y83JXrV7MPGlUxvmUUaQlOjpy3oKpXbv2btKkOTI3gnFW3n+/jZenNyivdav2YWHvnT79O1LHxYns7Ox+/Ya2a/uhj49vTk7OyT9+HdD/k65dejnYO3Tq2K1tmw9/3PldeY7ftu2H9euFwQFbtWiXkZEBt6dG9ZpSqbRFi7YREY/hxqcr0vfs/WHwoJHNm7eys7Vr1bIdyP2nXVvz8vJevXp59typ/v2Gwi7Ozi7hoyfK5ZbMYcEXAeMKGmrcqCkUjR0z2d7B8eDB3cgQXr5MmjJlTr26DRvUb9SzR7+oqGdpaallnE8ZRdpjJie/mj5zXK1a9T4dOxUZAgH1Gcm+STSDEI3rWalWtXA2o0relaOin2nfhgSHMgtPnjzMzc0Na/ietqhunQbgnKampb71+JUr+zMLNra28BpQpSrz1srSCu4fHBaqVFioXr2mdpegoOoKhSIu7nlCQhy89fML0BYFB9dgFu7euwUWFyTOvAWhwyndvnMTGUJgYBDoiVl2sHeEV/j5lXE+ZRQx55CTkz1z1nh7e4eF81eQpIEaIAiCg0aiYHpWLC2tdJYtMzIU2rdgJpkFhSIdXksGO16nJIOBRGVS7H6UvD0pKa/UH11g6gArK2t4zcrKTE1TP0xkrXmbX1RwtnBKoAloeuoeytHRCRkCGGbtMlHwGy7jfMooQpq5G3/e/xM0y57ldQAAEABJREFUpmvUqKW9dOWHLpgAg13MIERSQhjR1cyIjAHsga4utbi4usHrtKlzK1WqrLve3d0TvTM2NmpLmZWdpV0DzVN4dXZ2ZeaHzc7JLlakPiUXVysrq2VLv9Y9lISUIC7PhzkTvUXM22rVQkaPnDBrzkRot3wyNBzxADMIkVIZ81A/tPehxcMsQ6NNW3Xq4lPJVy6XwwI0p5g1r1+nwIdZW1ujdwbqR4lEcv/+7eoh+S2Bhw/vQY3p5ubOmM97924HB1WHBTCB129cYcwe7JWVlQW/hErePsxe8Qlxjg6GWURDz8fK2rq0IuZtk8bN69ZtMCZ8MvhhjcKagmlE5kYwzsq165euXL0ICxCs+ffW9XbtOpbcBgQHv2/4lYOLAK068JehPQ7+L2IDezv79u06/bRr28WLf6elp0GM5pfD+3r3HggqhBtcs2adHTs2QeMMHCYI1mhtPrgXjRo1Xb16SVJSYmrqm8NH9o8ZO/j334+id6aM8ymjSPcI3bv1ady42aIls6CGQeZGMAHtAf0+2bp1w6zZE+Fq9uzZ76NO3fVu1q/vEDAVu/fuuHnzKlReoTVqT5s2D7EERI7g05csmwN1sbe3z4D+w8BTZoogfrR27fLRYwaCOfywQxdw2OEHwxQtX7b26LGDi5fOfvDgbuXKfvATgvNHbFDG+ZRRpMuszxYNH/HxwUN7Bg4YhsyKGea+WT81ok5L57qtnMu5Pbi9EMr+39ff1a5dD2HMzZ6VkQ4ukr5TWZ4WzCzDwBBGwNBFp9tjCbMMA0MmZveeHXv27NBb5OcfsP6bbcisdOnaqrSizz77vHmzVqgCIIA2YkBA1bOnr6N3oEuXXq1bf6C3SCox/xXYsqXUjhboEkQ8g0BieZzU9M+rQORC2zPBQ6DrEgkILvpVzGMR8cNTgoZASCRdfBR+eErAaLr42L99eGAshhdgIWJ4ARYihhfgSZgwhlGQU4hl8CRMGMNQ52ISxwP2GExJsBAxvMAMQpRZkBILFoYoY8yC3JK0smb/9plBiBZyyesE9lMXYUxDbi7l7GmJ2MYMQ7L8QqwTojIRRoCkvlQqc+n3e7A/FMMMQmzT340kiOOb4xFGaBz//nmNMEfEAWbL13xgTWx6uqpSVbtKfpa5quIptQjdaS0KcnXrmelCHdSiUckBjkxy7pJ7ENoUzOrxndp4JkEUvQ6FKZyLLGv2Kjxoib10czcXbFaYUlrfwUueo/qg6gTV+qf1KJpeufCAqJTtdU9D+y00/4qeUf4XoQu+YpHPlEnysuiYh4qXsZkdh3n5BnOSR9GcGex//+FFXERmXi6lzC1zrCzNDNjRK0SkyelNlFzP3BpC3/oiC3opIkSdLbX5wpGegxT5uPJ9EK13HEtpX6r0o9EGjocpbXu960GmUgvCxlb6Xie3wPpcZfM0pxB5wq5du1JTU8eNG4cEyP79+9euXZuTk2NnZ2djYyOVSmGhUqVKgYGBo0aNQsKhogtx3bp1SqVyypQpSLCMHDny5s2b2kdFVSoV2DCKom7duoWEQ4V+kGnRokX29vaCViEQHh7u7u6ufSuRSECU3t6CGvVdkYU4adKk+vXrDx06FAmcsLCwevXq6dZs1tbWJ06cQIKiggpx0KBBffv27dKlCxIF0ByEdiGzDFVzbm7uqVOnkKCoiELs1KnTvHnzmjZtisRCQEBAmzZtmOXKlStfvnz5zJkzc+bMQcKhYgkRvOMmTZrs2LEjJCQEiYvhw4d7eXnZ2toePaqeWGf58uWtWrWCL3v+vIHTEpuJCuQ1R0ZGgoP5xx9/QHMeVQwgIDB9+nQXF5f58+cjflNRLOKNGzdmzpx5+vTpiqNCpJnhE6KMderUad269fXr7zRJAddUCIsILfcDBw5s3rwZVVTS09NnzJgBUW54RbxE/BZx37590HKvyCoEoLtl06ZNvr6+4Kjdv38f8Q+RW8SNGzcqFAremgHT8+LFC2iiNGjQYMKECYhPiNkiLl26VC6XYxXqAn0wEDSA/qRevXo9ffoU8QbRWsSpU6e2aNGie/fuCKOP6OhoMI3t27eHSALiAeK0iJ988kl3DQhTCn5+ftB6hvjOwIED4+PNP0hZhBYROu5WrFgRGhqKMOXg8ePH0HqBDk9QJDIforKIGRkZzZo127JlC1Zh+QkODobOGHBioI5OSUlBZkI8FjEmJmbIkCEnT55kUq1gDOX27dtgGseMGdOzZ09kckRiEeEiTp48+dy5c1iFRgMdMND/CTX1+PHjs7KykGkRgxAhXr1u3bpDhw4hzDsze/bsQYMGffDBByYe0Sj4qhn67q5evbpy5UqEYZUFCxaAXVy1ahUyCcK2iOCXREREYBVyweLFi6E/MCwsDCocxD0CtojLly93cXEZPXo0wnAJxL0tLS1Bl4hLhGoRwb8LCgrCKjQBUOE0adKkefPm4McgzhCkRfz5559TU1OF9dyu0MnOzp44ceLChQu1D8ewiyAtYlRUFHTbI4wJgdoZtAi/f8QNgpyoUyaTMUnjMaJBkEKUSqVYiCIDCxHDC7AQMbxAkM4KFqL4wBYRwwuwEDG8AAsRwwuwEDG8AAsRwwuwEDG8QKhdfHl5eQgjIrBFxPACLEQML8BCxPACLEQMLxCSEHv06BEVFUWSJDOqvH79+kxmm3///RdhBI6QBj2MGTPG2dkZxEcWAIqsXbs2wggfIQmxQ4cOAQEBumvs7Oz69euHMMJHYMPABg8e7OLion3r6+vbsWNHhBE+AhNiixYttClS5HI5tBoRRhQIb2DskCFDPD09kSbFUteuXRFGFHDoNT/5Nys3u7wdcSVSVpdM7Z6/nRxVC6ve+zH5uE2TNo+uZRbm+C66B/Ou+GGJglWlPMxNEsjaTuYfylV6bExpcCLE3Svi3iRnkySRVyI1fUkl5K8hNFnbi26g0Rih3Qzla4t2Rs3fC2yWFU2cjX5R7MiFbxmBErA5UfLTUSkJ5kmCkMjU5e5+lj3HCSzTrKBhX4jbF0ZZ2ct6Tw60skUC5UVM7t8HEw+vj+8+HmvRRLDcRtw6P9Ldz+ajkZWEq0LA3dei9xTfDAW1e+VzhDEJbArxn1+SKYpo0csNiYKuY31SX+WlxFWUpJnmhU0hPn+c5eAiqpmDLa0kV/58iTDcw6YQszLzpCKbwZqkM9JyEYZ72HRWVHlImadCIkKZi5TZovpGvEWQw8Aw4gMLsSwgGEmQBMJwD5tCJEgksrsGIXGawl6zKWDTWaEpJLK7RkhoUir+3Op8gE2LSBM0ITKLqCIoJYUw3MNq1QxKFF89hpuIJoHlNiK+axjjYLVqppAIG/bYVzEJ7IZvaBHeNmzkTQKrLqE67IbERf4oSQzXsFo10+KziARB4LrZFLDqrCDxWUTcRjQRFSha26NX+/iEOIThJaw6KzwO3yQmJrx58xoZAW4imgRWLaLh4ZvIyKf/++bLocN6d+jYNHzMoCNHD2iLHjy4Ozp8YKfO7382e+L9+3cmTBrx9drlTFFKSvLSZXP7DejcvWe7ZcvnP38ezaz/5fDPPXt/EBMTNWzEx63bNhwxqt/vJ4/B+n9vXe8/sAssDBzUbfVXS1H5IShCZJ1FfIVViwjtegNv24Zvv0pMjJ86dS7cbxAQiNLDw6tJ42bZ2dlz5k0JDqq+eNHqtPTUtf9bkZLyKjCgGuyiUqmmTAvPyFDMmL6gWtXgvft+HPfp0E2bfqrk7SOTyRSK9G/WrZwxbX716jV3/rR15arF9eqG1avbcPmytbPnTt710xFvLwOyvBI4Rm8qWA7fGNrHN3/+8lWrvq1fT62Vbl17g/KuXrsI6y9fOZ+a+iZ89CRPT6+gaiGjRo5PSkpkdrl79xZIds7sJY0bNXV2dhk7ZrK9g+PBg7uZ0ry8vKFDRteoUQvOpcMHncGRj4gwPt21OkSvwt6KKWDVIhrRs0LThw7tvXL1grZ69dJYrMjICFtb24CAqsxKkKmdXX6C5rv3boHlA+0yb0Fwdes0uH3npvaQISGhzAKzC9hI9C5gi1iAlRWH8w6Y01mhKGrWnEl5eblg8OqC1GztoCHIFKUr0q2tbXQ3dnR0YhZAWGD2oAmotxRppIlYBBvEArKyshBnmNMiPvnv0aNH91ev+rZB/UbMGhCZm6s7LFjKLXNzizy1lJyc/zSdi4sr/DSXLf1at1RCShAXQKuXxOMRTQHL4xENMonQCoRXRnlAVNQz+KviHwjLlSpVhmgLeMfQCkQatzczM5PZLDAwCH6a7u6e4J0wayA66OjghDiAILDTbCLY/LmTBt41f78AqVS67+edaelp4H+sW78qrGGTxKQEKGrSuLlEIoE1GRkZsXHPd+783s0tX69gPhs1arp69RJwX0DKh4/sHzN28O+/Hy37syr7+sPruXOn/jPEd8HOislg+VEBg5xmDw/PuXOWPnh4t1v3NhCsGTni065dez98eA/CilD/Tpk8G1yQXn0++HLl5wMGDLOyspZKZcyOEItp2bLd4qWzIY546Je97dp17NnzLfPGgvn8sEOX7Ts27d69HWH4B0GzN6j6u7mRjm6yD4f5IDaIi48Ft9de4/nCSXbu2nL4J2N79eqPTMi+1VF2TmTfqb4Io5mZctasWTVq1EAcwPLoG5qlaAfUuRCmrhoYNGLEp05Ozlu3biAJslWr9sjEiPDRB57CdkCbpWiHg4Pjii/+B8JesHB6ePjA9PS0Det3QH2NTAsN/hduI5oEcwe0Swf66NZ8tQmZFXUPH37A3iTgh6fKQu1+4QfsTQLb0VpxRX/pknN7Y7iB9blvRHXbiDLmfcewCtuPk+KKDGMUbLcRcT2GMQqWLSKuxzDGwapFhG4aUlRKJKUIzwZmGth9VIAgKFHVzZQS4dnATAOe+wbDC9i1iAiDMQ42hSizIKQybkZKmwkLOWFpKUMY7mFTiHJrSV6WqFpUKhVt7YjnuzcFbLqEQbUdUl+XNy+uIMjJotr2cEcY7mFTiA0/tLewIH7bGo9EwYE1MW6VLSVCzm4pIFgOkn2y0E+lpA598/zpvwokWO6cT9u3KtKvhlXvCThNrolgvwHUf6bPwXUJV06+uHQ8SaksEc/RO5yF1jdXSakDX/QfQv/WpR2klPWwjpQQpJQOrOXQpq9I0qwKAk5a4r0meKn/U6AsVYlEdprM8nSxyUkIba56onB0PlG4HPP8+Zcrlm/Y8C1SP7SqftpYI1ydeUF1R8mUHDGjm7O+2PpiKyXIykqyZeuWRMoKocEIYyq4dAltkRViJ5rz+K871UL9rRxMFBsaPXr08ePHEcaECKMjtWvXrrNnz0Ym5KOPPoLX5cuXR0REIAz3CEOIGRkZFGWGCOXMmTOXLjVkPkWMsQhAiOnp6Z07dybNMQeNRCLZsWMHLFy8eBFhuEQAQnz8+HGLFi2QWXF1dR08GPsuHCKA/quGGpBZCQoKmjNnTnJyslQqdXBwQBi2EYBFjIuLy1dEWsgAABAASURBVM7ORuamevXqLi4u4Lvs3bsXYdhGAELs1asX2CHEDxo0aBAbGxsTE4MwrMJ3IUZHR3fs2JE/QgSmT59ua2v79OlThULA3Zh8g+9C9PPzW7hwIeIZzs7OPj4+4Mu/fPkSYdiA70IEwwMuAuIfcrn83Llzz549KzbFMsY4+C7EKVOm5OTkIL7SuHFj6DmfMWMGwrwbvBZiWloaBG68vXk9FgtMY6dOnXbt2oUw7wCv44j29vYLFixAvKd169bQCQkLly5deu+99xDGcHhtEaFPJTIyEgkBGxt1Vpjffvvt9OnTCGM4vBbiihUrhBUiWbx4sUyGn/ozBv4KEZyA4ODgWrVqIUHBdIt/+umnSUlJCFNu+CtEgiBmzZqFhMmqVavWrFmDMOWGv0K8f//+zZs3kTCxtrb+8ssvYQGP9C4n/BXi9u3bIXyDBI6Tk9OkSZMQ5m3wN3xTv359CBcjgdO0aVNm2Bj0D7m4uCBMKfBXiAMGDECiIDRUnT/6zJkz4FB3794dYfTB06r54cOHJ06cQCKiT58+9+7dwx3TpcFTIf7zzz9IdMybN08ikZw7dw4JE39/fzh/xA08FSK0DiGIiEQH3EiIjPbt2xcJkD///LNKlSqIG3jaRqxTpw4SKeCyfPHFF9nZ2ZaWlkg4xMTEeHp6WlhYIG7gqUWEpv3Vq1eRSAkMDJTL5T/88AMSDhEREdWqVUOcwVMhPtCAxAv0G/Xr169Vq1ZIIPz3339Vq1ZFnMHTqrl169bctYt5AhjFs2fPIs08FszgHT4DFrFTp06IM3hqESH2FhISgsQOoZmMb9u2bXFxcYjfgEWsiFXz+fPnRRnB0cuECROWL1+OeAy4Vq9evfLx8UGcwVMhPnnyBMK/qMKwfv16pKn+EC+BE+O0gYh420Zs1qxZXp6o5oUvD6dPn05PT69Xrx7iGVzXy4i3FhGi2TVr1kQVjPDw8EuXLiH+wXXsBvFWiNevX4c4Pqp4jBs3DmnCqIhPgBAh9om4hKdCfPbsmXBHxb478fHxf//9N+INFbdqDgsLa9++PaqoDBo0KCsrC/GDFy9eQG+kvb094hKeChE613nYZjclHTp0gNfNmzcjc2OCBiLirRDv3r3766+/ogqPl5fXyZMnkVnhunOPgadCjI2NvXLlCqrwdO3aldMwcnkwQRAR8VaItWrV6tKlC8IUPGkwceJEZCZM4KkABE3jrPMC4OrVq9DJph12AJZSLpfv378fcQ84jteuXUMcw1OLCL/CgwcPIkwBjTRAvwss9+zZE+I74MxeuHABcYwJIogMPBViUlJSxRn0UE5cXV2tra3BPjEzeCsUilOnTiGOMU29jHgrRPjyffr0QZiifPzxx9qmFEEQEFtgbCR3mCZ2g3grRA8Pj2bNmiFMUaKjo3XfJiYmcl1vmMZlRrwVItQ+eA7WYoCDIpPJdJ1L6H3hOspomiAi4q0QX79+zQyjx2g5evTozJkz33///cqVK0OHGyiSJEnolIeYK+KG1NTUnJwcd3d3xD08Dd+kpKTcvn27devW0CqCa3HkyBGEKeDNmzeHv03IeE3SKpKmCe16de51zTtak10d5adhJ7RvmYztuguaxOlEkZ0LVxVBpxzRFCJ0LZjODrqbASRJkBLC1l7aI9zX1g2VAb8Gxi5YsODYsWPwQy9Icq/+0bu5uSGMDsc3p9NKm4ZtHarWdFBRqnzJIY1B0WgiXwwgCpJAFF2oSeYtULBAk4hg0g8X6rZAoFoDRRRITf2q2QYVLOS/5n944UG0WEjSEnLvXEre+eXTEUsCLaxK/VL8EuKQIUPAEMbFxTG/V0aLgps0llO2LYy2c7TsOsajYAXfn3W0CrBoH+AFC9sXRXYb7e8ZQOjdjF9tRGgXF5uKDoJnUDsjjIbzh1MoFf3hcA8kQHxD7E78UGoOQ945K4MHD/b19WWWKYoKCgoye45c/hB5X+HkIUfCpHkP1+wMJVLpL+WdEMElbNGiBeNCOTk5iWaWRFbIyVZZ2ws6awEdF6V/Yj4+hm/69u1bpUoVMIfQy4nz5+iSl0Pl5Qh4hkWVCtFK/SbxXZ2VxGe5EbcVyYlZmQpKRdG0CvxcgqJoxtlQu70SpHbsQPA0ofGFC7wuJqyg44ShAs8M1rSsMq+eazo0EHcujyY1xRRV0LVFqh0zimK8mcLdCZ1tmGAFKghtyCygVGJtL3HxtKjTwsnGQQBZqkWJ+v7q91WMFeKja4rrf6akJufB7ZZISJrUOLgEQapjAVrvnVJrUSfGpN1dEwegSpxUYfDAQuLo6uhIK1FGKqX5AkW2pAiaLBr91IbKtG9ptWK12oWTU715lRfzKOPGmRQ4YffKlm0/9nTyEt70OjQikKApJWxtsBAfXlX8c/iFMpeW21p4B7s6+dgiofEyIvXNK8WuVZGWNpIhn/lb2Arp1pKCHj9aNNyti2FC3PlFTFqK0tHDtlJNAU+Q71bVAf5gIfJawub5EZWq2PScyOsEqLrQgjaIRGkG0RBnZeP0Z3k5ZGhbP0GrUJcqYV61Pgh4EZ/z/bwohOEemmmr6aO8Qlw/NcI1wCmgiRcSHSEtfWVWVlvnRSMhQAi5jViGs1IuIW6Y/rRaA1+3Ktw+Ym1G/Oq7Sq0tNs96hngOodsHLFCM7eLb+NlTz6rOcmeRz9/qV8/dytFy+yJ+20WBi5CmS23ivkWIO5fFWFjJXfxEawt18a3jkZtDn9iO09tyBUEUHZujQ1lCvH9RkfZaGdhYhO3C0ghuVvnZXX6nKhd29KZUyhLi+WMvnDyFFyZ8J0hk7WCxe+VzxEsk0KskFbizUkpRqUK8fyldlUt7h1a4hJoBDb1TEnMQL1FBb5RKwCaRRsjgNuKNP1Ms7fk74ujW3T+nz2+syHiNWIdEUgvJr98lIIyG7j3b/bjze8QGahWSBnrN6W+Urv5OqEJi62ydGJ2NRMGixbNO/MaXJ37UxpwyxFmJvpcF6rV3F1KyOBZxreKUk00hUfD4sTASeOnva35yO00i5XCsVFTMnT/Ofv889oGtjVP14OYftB5paalOvXTh8v5Tf20bO3zjj3tnJ7145uVRtUXT/mH1OzN7/fr7uuu3T8gtrOvV7uDu6os4w9JWHTSNupvlX8sK8QlCQhMSA+5L67bqwe2rVi/ZuOnrY0fOwfKFC3/98OOW6JhIBwfHqlWDJ034zMPDk9m4jCIGiAIePLTn5Mlfn8dG+/lWadiwyfBhYw1KEGaws5KaopRIuYpgv0p+vnnHhLy8nPGjvx864MuEpP82bhurUimhSCKVZWWlHz6++uPuc1Ytvly7ZpufDy99/SYRii5ePXjx6oGeH82YFL7dxcn71NmtiEtICRn/jHe1M60iaJUBpvr3E+pZmmZMn8+o8PqNKws+n/HBBx/9vPfEwvkrkpIS1n6zgtmyjCIthw7t/WnXtt69Buzd/WuXLr2Onzi8d9+PyBCKjdbTRb8Qc7NV3HVp3rz9u1Qi+6T/lx5u/p7uAX26zY1LeHzv4V9MqUqV1771SL/KtSD42bDuR/ArjEt4AuvPX/q5dmhbkKa1tT3YyKoBHD/IQtOKdJ76zkazbfvGFu+3ASWBzQsNrT1u7NTLl88/0tTdZRRpuX3nZnBwjQ4dOjs6OnX+qMeG9TsaNzJsWhhmNLTeolLsvIrDuCnUy5V9atjYODJvnZ28XJx9IqNvaTfwrRTKLFhbqXt0srLTQY6vUp57uFfRbuPjzW2mPkLzGDnveDfr8OzZfyEhodq3wUE14PXRo/tlF2mpWbPOjRtXVq5a/PvJY6lpqZW8fapWDUIGYth4RKmcROmlPG71zmRlK57HPYDgi+7KtPRk7TJR4mSzczIoSiWXW2vXWFhw23oDEVrb8e8xpXcwDwqFIicnRy4vdECtrdXXMzMzo4wi3SOAvbS2trlw8a8vVy6SSqWtWrUPHzXR1dWA6Q9oZOAIbTsnWUqSEnGDnZ1LFb+6HdqM1l1pY+NQxi6WchuSlOTlFTbacnIzEZfQFO3qzbswqvp5HcJIq2hpqdZZdnZh1owMjc5cnF3LKNI9AkmSUCPDX1TUs5s3r+74cUtGhuKLpV+jcpM/J4U+9AvRN8Sauy5Xb49qN26fCPCvB1+MWZP44pmbS1leMFx9J0evqJi7LQvaJA8fczhZKqWCP6p6Y951b0Jrwei5isCGBQdVv3//jnYNsxwQWK2MIt0jgL8cFFS9SpVAf/8A+EtXpB8/8QsyBJo2sGcltIkdRVE5Ck7SMkJEBg5+9Levc3OzX7yM/vXk+q/WD0hIektizjo12919cBY6VGD5zD8/RsdymLs08UkyKRH4M0oa5HK5m5v79euX/711XalU9uje9/yFcwcP7klLT4M1325cU79eWLWqwbBlGUVaTp/5HTzrixf/hgYiuDL/nD9TM7QOMohSB9+U/syKpbU04UmKf332Z7cAt3f6+N1n/9m5dtPQFy+jfH1C+3Sf+1bno13LYRkZrw+f+Oqnn+dCzd614+Td+xdwNJVZ2osMFy+hTqhQjIEDhm/fsenqtYt7dv8K0ZmXr17s279z/bdfQYywYYMmo0aOZzYro0jLtKnz1m9YPXf+VFh2dnaBOrpP70HIEMoYoV3qtHR/HXx571JaaFt/VPG4dyqy33RfV28LxDM2znxaqapV676CedSrGDs+j+gxppJPsB5Hs9QwfctebtBgT4nhdopmHhJ1M8nCUsJDFYqbsh4nrVbX7umdZGdfO72lb1KTVq/XPzGNldw2K0e/r+PpFjB+9HeIPeYta1taEfTWSCR6vqC/b+2Rg0v19RTJmV1GVEK8hFBPZCDOh6fKEmKHIR6bPsuIvZfso+/5UXs7t7lTD+vdMU+ZK5OWYlHYvo6lnQMqXYgQCSptl6cX4x3dLfxC+dXFrAXqKEEnaCrjcdK3PGA/5ouA9TMi9AoRgi9WVvqNpSlvY2nnYASvYtJzsnJHLDFFfhvjUE/xI/QpR0rhbUM5JKhlT48HZ6KQ2IHYYdKT5HGr+atCpInDCf1xUuOfa67V3K7XeJ97p6KQeMl4lfPgTOSnK3mtQnHwTjM9ePjJ2w9wh6BG0n9vkOiIvpkU+W/8+DVVeT8dtRrBzwZWCuWdhCm4oV3lENsfFkemJil861WytBHD5XgTmxH/5JVUhtQqFAiEwJ8nZWE2MGtbYuzKgCObEp5ejpbKJE4+Du4BQn3wPv5eStqLdPBBqzdyaP2xK8KYBMKILr7S6DZG/bz94W/jk2Jev4xMkUhJUiaRW0olMlIna0fpZ0G8bSwTkT+XrOaUC54/VP+OqPzhvYW767zRTfVR4viEhKSUtDJXlZuVC6+UipLLpYG17doPxhlcTEoZd97IGWO7j1P3MiVF5936O+VlbHZ2Zm5WOkWppy4u3IaZw1j7lijIS6ObMEZbpLucH2+ii2eXKb6ANEmBqML1RNH8R9pXqUwdBialhI2dxDvApnkYSYnRAAABTklEQVQXNwtrhOEV7zSHtoefrMNgQeb8wPANfmWewpQNKSM4fbqSa0gJSZQSqMFCFBKWFlJVrpCFSNDObvrH1+FED0LCrbI8OUmoU1DcOvtaakFYOesvxUIUEp2Ge+RmqZ7eEqQWH99IDW1U6iQ2PM3XjCmDzTOeVQqxa9lbMLGnp7cyL59IbN3LLbhRqSNUsBAFyfbPo7MylRIJqczRCdyq02/RNJUfxMoPdZWMsBZNy1WYjlnfSu0BCwNnEs0T3yVDbxIaqYhiH0fK1McgCSK4oX3L3mVNcYiFKFQUqerKLidD5wE3QpMQvECZBZFUktbEWpnk1+oCktBMH1CQlosg8hPaEzRdfN/ClYU5xyUkrY4YF0iNIDRvNAJV6ehSsySREK4+1oG13z4wEAsRwwtw+AbDC7AQMbwACxHDC7AQMbwACxHDC7AQMbzg/wAAAP//K3p4RAAAAAZJREFUAwDHZPXhrOAZOwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e8e76-5d43-44cd-bf01-a39212cedd8d",
   "metadata": {},
   "source": [
    "We'll also define a utility to render the agent outputs nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16636975-5f2d-4dc7-ab8e-d0bea0830a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream, output_messages_key=\"llm_input_messages\"):\n",
    "    for chunk in stream:\n",
    "        for node, update in chunk.items():\n",
    "            print(f\"Update from node: {node}\")\n",
    "            messages_key = (\n",
    "                output_messages_key if node == \"pre_model_hook\" else \"messages\"\n",
    "            )\n",
    "            for message in update[messages_key]:\n",
    "                if isinstance(message, tuple):\n",
    "                    print(message)\n",
    "                else:\n",
    "                    message.pretty_print()\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84448d29-b323-4833-80fc-4fff2f5a0950",
   "metadata": {},
   "source": [
    "Now let's run the agent with a few different queries to reach the specified max tokens limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ffff6c3-a4f5-47c9-b51d-97caaee85cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:25:55 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:25:57 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:26:07 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\n",
    "result = graph.invoke(inputs, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb186da-b55d-4cb8-a237-e9e157ab0458",
   "metadata": {},
   "source": [
    "Let's see how many tokens we have in the message history so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ba0253-5199-4d29-82ae-258cbbebddb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = result[\"messages\"]\n",
    "count_tokens_approximately(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "812987ac-66ba-4122-8281-469cbdced7c7",
   "metadata": {},
   "source": [
    "You can see that we are close to the `max_tokens` threshold, so on the next invocation we should see `pre_model_hook` kick-in and trim the message history. Let's run it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c53429-90ba-4d0b-abb9-423d9120ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: pre_model_hook\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's it known for?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "New York City is known for a variety of iconic landmarks, cultural institutions, and vibrant neighborhoods. Some of the most notable features include:\n",
      "\n",
      "1. **Statue of Liberty**: A symbol of freedom and democracy, located on Liberty Island.\n",
      "2. **Times Square**: Known for its bright lights, Broadway theaters, and bustling atmosphere.\n",
      "3. **Central Park**: A large public park offering a natural oasis amidst the urban environment.\n",
      "4. **Empire State Building**: An iconic skyscraper offering panoramic views of the city.\n",
      "5. **Broadway**: Famous for its world-class theater productions and musicals.\n",
      "6. **Wall Street**: The financial hub of the city, home to the New York Stock Exchange.\n",
      "7. **Museums**: Including the Metropolitan Museum of Art, Museum of Modern Art (MoMA), and the American Museum of Natural History.\n",
      "8. **Diverse Cuisine**: A melting pot of cultures, offering a wide range of international foods.\n",
      "9. **Cultural Diversity**: Known for its diverse population and vibrant cultural scene.\n",
      "10. **Fashion**: A global fashion capital, hosting events like New York Fashion Week.\n",
      "\n",
      "These are just a few highlights of what makes New York City a unique and exciting place to visit or live.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "where can i find the best bagel?\n",
      "\n",
      "\n",
      "\n",
      "17:26:17 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Finding the \"best\" bagel in New York City can be subjective, as it often depends on personal taste preferences. However, several bagel shops are frequently mentioned as top contenders:\n",
      "\n",
      "1. **Ess-a-Bagel**: Known for its large, chewy bagels and a wide variety of spreads.\n",
      "2. **Russ & Daughters**: Famous for its traditional bagels and high-quality smoked fish.\n",
      "3. **Absolute Bagels**: Popular for its fresh, fluffy bagels and authentic taste.\n",
      "4. **Murray’s Bagels**: Offers a classic New York bagel experience with no toasting policy.\n",
      "5. **Tompkins Square Bagels**: Known for its creative cream cheese flavors and hearty bagels.\n",
      "6. **Bagel Hole**: Celebrated for its smaller, denser bagels with a crispy crust.\n",
      "7. **Black Seed Bagels**: Offers a unique Montreal-style bagel with a New York twist.\n",
      "\n",
      "These spots are scattered throughout the city, so you can find a great bagel in various neighborhoods. It's always a good idea to try a few different places to find your personal favorite!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\n",
    "print_stream(graph.stream(inputs, config=config, stream_mode=\"updates\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe0399-4e7d-4482-a4cb-5301311932d0",
   "metadata": {},
   "source": [
    "You can see that the `pre_model_hook` node now only returned the last 3 messages, as expected. However, the existing message history is untouched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ecfc310-8f9e-4aa0-9e58-17e71551639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_messages = graph.get_state(config).values[\"messages\"]\n",
    "assert [(m.type, m.content) for m in updated_messages[: len(messages)]] == [\n",
    "    (m.type, m.content) for m in messages\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035864e3-0083-4dea-bf85-3a702fa5303f",
   "metadata": {},
   "source": [
    "## Overwrite the original message history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a4fd5-a2ba-4eca-91a9-d294f4f2d884",
   "metadata": {},
   "source": [
    "Let's now change the `pre_model_hook` to **overwrite** the message history in the graph state. To do this, we’ll return the updated messages under `messages` key. We’ll also include a special `RemoveMessage(REMOVE_ALL_MESSAGES)` object, which tells `create_react_agent` to remove previous messages from the graph state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c2a65b-685a-4750-baa6-2d61efe76b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:26:17 langgraph.checkpoint.redis INFO   Redis client is a standalone client\n",
      "17:26:17 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "17:26:17 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "17:26:17 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_387/2986813568.py:27: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  graph = create_react_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "\n",
    "def pre_model_hook(state):\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=384,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    # NOTE that we're now returning the messages under the `messages` key\n",
    "    # We also remove the existing messages in the history to ensure we're overwriting the history\n",
    "    # highlight-next-line\n",
    "    return {\"messages\": [RemoveMessage(REMOVE_ALL_MESSAGES)] + trimmed_messages}\n",
    "\n",
    "\n",
    "# Set up Redis connection for checkpointer\n",
    "REDIS_URI = \"redis://redis:6379\"\n",
    "checkpointer = None\n",
    "with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "    cp.setup()\n",
    "    checkpointer = cp\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    # highlight-next-line\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd061682-231c-4487-9c2f-a6820dfbcab7",
   "metadata": {},
   "source": [
    "Now let's run the agent with the same queries as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831be36a-78a1-4885-9a03-8d085dfd7e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:26:30 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:26:39 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Update from node: pre_model_hook\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's it known for?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "New York City is famous for its bagels, and there are several places renowned for serving some of the best. Here are a few top spots where you can find delicious bagels in NYC:\n",
      "\n",
      "1. **Ess-a-Bagel**: Known for their large, chewy bagels with a variety of spreads and toppings.\n",
      "2. **Russ & Daughters**: A classic spot offering traditional bagels with high-quality smoked fish and cream cheese.\n",
      "3. **H&H Bagels**: Famous for their fresh, hand-rolled bagels with a perfect balance of crust and chewiness.\n",
      "4. **Murray’s Bagels**: Offers a wide selection of bagels and toppings, known for their authentic New York taste.\n",
      "5. **Absolute Bagels**: A favorite on the Upper West Side, known for their fresh and flavorful bagels.\n",
      "6. **Tompkins Square Bagels**: Offers creative cream cheese flavors and a cozy atmosphere.\n",
      "7. **Bagel Hole**: Known for their smaller, denser bagels with a crispy crust.\n",
      "\n",
      "Each of these places has its own unique style and flavor, so it might be worth trying a few to find your personal favorite!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "where can i find the best bagel?\n",
      "\n",
      "\n",
      "\n",
      "17:26:47 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "New York City is famous for its bagels, and there are several places renowned for serving some of the best. Here are a few top spots where you can find delicious bagels in NYC:\n",
      "\n",
      "1. **Ess-a-Bagel**: Known for their large, chewy bagels with a variety of spreads and toppings. Locations in Midtown and Gramercy.\n",
      "\n",
      "2. **Russ & Daughters**: A historic appetizing store on the Lower East Side, famous for its bagels with lox and cream cheese.\n",
      "\n",
      "3. **Absolute Bagels**: Located on the Upper West Side, this spot is popular for its fresh, fluffy bagels.\n",
      "\n",
      "4. **Murray’s Bagels**: Known for their traditional, hand-rolled bagels. Located in Greenwich Village.\n",
      "\n",
      "5. **Tompkins Square Bagels**: Offers a wide range of bagels and creative cream cheese flavors. Located in the East Village.\n",
      "\n",
      "6. **Bagel Hole**: A small shop in Park Slope, Brooklyn, known for its classic New York-style bagels.\n",
      "\n",
      "7. **Leo’s Bagels**: Located in the Financial District, offering a variety of bagels and toppings.\n",
      "\n",
      "Each of these places has its own unique style and flavor, so it might be worth trying a few to find your personal favorite!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "messages = result[\"messages\"]\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\n",
    "print_stream(\n",
    "    graph.stream(inputs, config=config, stream_mode=\"updates\"),\n",
    "    output_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a0604-3d2b-48ff-9eaf-d16ea351fb30",
   "metadata": {},
   "source": [
    "You can see that the `pre_model_hook` node returned the last 3 messages again. However, this time, the message history is modified in the graph state as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394f72f8-f817-472d-a193-e01509a86132",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_messages = graph.get_state(config).values[\"messages\"]\n",
    "assert (\n",
    "    # First 2 messages in the new history are the same as last 2 messages in the old\n",
    "        [(m.type, m.content) for m in updated_messages[:2]]\n",
    "        == [(m.type, m.content) for m in messages[-2:]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee186d6d-4d07-404f-b236-f662db62339d",
   "metadata": {},
   "source": [
    "## Summarizing message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa6e4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langmem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e53e0f-9a1e-4188-8435-c23ad8148b4f",
   "metadata": {},
   "source": [
    "Finally, let's apply a different strategy for managing message history — summarization. Just as with trimming, you can choose to keep original message history unmodified or overwrite it. The example below will only show the former.\n",
    "\n",
    "We will use the [`SummarizationNode`](https://langchain-ai.github.io/langmem/guides/summarization/#using-summarizationnode) from the prebuilt `langmem` library. Once the message history reaches the token limit, the summarization node will summarize earlier messages to make sure they fit into `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9540c1c-2eba-42da-ba4e-478521161a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:26:49 langgraph.checkpoint.redis INFO   Redis client is a standalone client\n",
      "17:26:49 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "17:26:49 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "17:26:49 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_387/3065461601.py:33: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  graph = create_react_agent(\n"
     ]
    }
   ],
   "source": [
    "# highlight-next-line\n",
    "from langmem.short_term import SummarizationNode\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "from typing import Any\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "summarization_node = SummarizationNode(\n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=summarization_model,\n",
    "    max_tokens=384,\n",
    "    max_summary_tokens=128,\n",
    "    output_messages_key=\"llm_input_messages\",\n",
    ")\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    # NOTE: we're adding this key to keep track of previous summary information\n",
    "    # to make sure we're not summarizing on every LLM call\n",
    "    # highlight-next-line\n",
    "    context: dict[str, Any]\n",
    "\n",
    "\n",
    "# Set up Redis connection for checkpointer\n",
    "REDIS_URI = \"redis://redis:6379\"\n",
    "checkpointer = None\n",
    "with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "    cp.setup()\n",
    "    checkpointer = cp\n",
    "\n",
    "graph = create_react_agent(\n",
    "    # limit the output size to ensure consistent behavior\n",
    "    model.bind(max_tokens=256),\n",
    "    tools,\n",
    "    # highlight-next-line\n",
    "    pre_model_hook=summarization_node,\n",
    "    # highlight-next-line\n",
    "    state_schema=State,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eccaaca-5d9c-4faf-b997-d4b8e84b59ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:26:52 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:26:54 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:26:58 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'summarized_message_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m result = graph.invoke(inputs, config=config)\n\u001b[32m      6\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [(\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms it known for?\u001b[39m\u001b[33m\"\u001b[39m)]}\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [(\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhere can i find the best bagel?\u001b[39m\u001b[33m\"\u001b[39m)]}\n\u001b[32m     10\u001b[39m print_stream(graph.stream(inputs, config=config, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langgraph/pregel/main.py:3050\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3048\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3050\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langgraph/pregel/main.py:2633\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2632\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2643\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langmem/short_term/summarization.py:832\u001b[39m, in \u001b[36mSummarizationNode._func\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    830\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | BaseModel) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    831\u001b[39m     messages, context = \u001b[38;5;28mself\u001b[39m._parse_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     summarization_result = \u001b[43msummarize_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrunning_summary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrunning_summary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens_before_summary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_tokens_before_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_summary_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_summary_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43minitial_summary_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_summary_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexisting_summary_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexisting_summary_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfinal_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinal_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prepare_state_update(context, summarization_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langmem/short_term/summarization.py:445\u001b[39m, in \u001b[36msummarize_messages\u001b[39m\u001b[34m(messages, running_summary, model, max_tokens, max_tokens_before_summary, max_summary_tokens, token_counter, initial_summary_prompt, existing_summary_prompt, final_prompt)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msummarize_messages\u001b[39m(\n\u001b[32m    338\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[AnyMessage],\n\u001b[32m    339\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     final_prompt: ChatPromptTemplate = DEFAULT_FINAL_SUMMARY_PROMPT,\n\u001b[32m    349\u001b[39m ) -> SummarizationResult:\n\u001b[32m    350\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Summarize messages when they exceed a token limit and replace them with a summary message.\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m    352\u001b[39m \u001b[33;03m    This function processes the messages from oldest to newest: once the cumulative number of message tokens\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    443\u001b[39m \u001b[33;03m        ```\u001b[39;00m\n\u001b[32m    444\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m     preprocessed_messages = \u001b[43m_preprocess_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrunning_summary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrunning_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens_before_summary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens_before_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_summary_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_summary_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m preprocessed_messages.existing_system_message:\n\u001b[32m    454\u001b[39m         messages = messages[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langmem/short_term/summarization.py:145\u001b[39m, in \u001b[36m_preprocess_messages\u001b[39m\u001b[34m(messages, running_summary, max_tokens, max_tokens_before_summary, max_summary_tokens, token_counter)\u001b[39m\n\u001b[32m    143\u001b[39m total_summarized_messages = \u001b[32m0\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m running_summary:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     summarized_message_ids = \u001b[43mrunning_summary\u001b[49m\u001b[43m.\u001b[49m\u001b[43msummarized_message_ids\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;66;03m# Adjust the summarization token budget to account for the previous summary\u001b[39;00m\n\u001b[32m    147\u001b[39m     max_tokens_to_summarize -= token_counter(\n\u001b[32m    148\u001b[39m         [SystemMessage(content=running_summary.summary)]\n\u001b[32m    149\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'summarized_message_ids'",
      "During task with name 'pre_model_hook' and id '38a6ec6c-13cc-a2ff-ed87-b517db665815'"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\n",
    "print_stream(graph.stream(inputs, config=config, stream_mode=\"updates\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caaf2f7-281a-4421-bf98-c745d950c56f",
   "metadata": {},
   "source": [
    "You can see that the earlier messages have now been replaced with the summary of the earlier conversation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
