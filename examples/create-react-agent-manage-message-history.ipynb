{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992c4695-ec4f-428d-bd05-fb3b5fbd70f4",
   "metadata": {},
   "source": [
    "# How to manage conversation history in a ReAct Agent with Redis\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - [Prebuilt create_react_agent](../create-react-agent)\n",
    "    - [Persistence](../../concepts/persistence)\n",
    "    - [Short-term Memory](../../concepts/memory/#short-term-memory)\n",
    "    - [Trimming Messages](https://python.langchain.com/docs/how_to/trim_messages/)\n",
    "\n",
    "Message history can grow quickly and exceed LLM context window size, whether you're building chatbots with many conversation turns or agentic systems with numerous tool calls. There are several strategies for managing the message history:\n",
    "\n",
    "* [message trimming](#keep-the-original-message-history-unmodified) - remove first or last N messages in the history\n",
    "* [summarization](#summarizing-message-history) - summarize earlier messages in the history and replace them with a summary\n",
    "* custom strategies (e.g., message filtering, etc.)\n",
    "\n",
    "To manage message history in `create_react_agent`, you need to define a `pre_model_hook` function or [runnable](https://python.langchain.com/docs/concepts/runnables/) that takes graph state an returns a state update:\n",
    "\n",
    "\n",
    "* Trimming example:\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langchain_core.messages.utils import (\n",
    "        # highlight-next-line\n",
    "        trim_messages, \n",
    "        # highlight-next-line\n",
    "        count_tokens_approximately\n",
    "    # highlight-next-line\n",
    "    )\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    from langgraph.checkpoint.redis import RedisSaver\n",
    "    \n",
    "    # This function will be called every time before the node that calls LLM\n",
    "    def pre_model_hook(state):\n",
    "        trimmed_messages = trim_messages(\n",
    "            state[\"messages\"],\n",
    "            strategy=\"last\",\n",
    "            token_counter=count_tokens_approximately,\n",
    "            max_tokens=384,\n",
    "            start_on=\"human\",\n",
    "            end_on=(\"human\", \"tool\"),\n",
    "        )\n",
    "        # You can return updated messages either under `llm_input_messages` or \n",
    "        # `messages` key (see the note below)\n",
    "        # highlight-next-line\n",
    "        return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "    # Set up Redis connection for checkpointer\n",
    "    REDIS_URI = \"redis://redis:6379\"\n",
    "    checkpointer = None\n",
    "    with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "        cp.setup()\n",
    "        checkpointer = cp\n",
    "        \n",
    "    agent = create_react_agent(\n",
    "        model,\n",
    "        tools,\n",
    "        # highlight-next-line\n",
    "        pre_model_hook=pre_model_hook,\n",
    "        checkpointer=checkpointer,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "* Summarization example:\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langmem.short_term import SummarizationNode\n",
    "    from langchain_core.messages.utils import count_tokens_approximately\n",
    "    from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "    from langgraph.checkpoint.redis import RedisSaver\n",
    "    from typing import Any\n",
    "    \n",
    "    model = ChatOpenAI(model=\"gpt-4o\")\n",
    "    \n",
    "    summarization_node = SummarizationNode(\n",
    "        token_counter=count_tokens_approximately,\n",
    "        model=model,\n",
    "        max_tokens=384,\n",
    "        max_summary_tokens=128,\n",
    "        output_messages_key=\"llm_input_messages\",\n",
    "    )\n",
    "\n",
    "    class State(AgentState):\n",
    "        # NOTE: we're adding this key to keep track of previous summary information\n",
    "        # to make sure we're not summarizing on every LLM call\n",
    "        # highlight-next-line\n",
    "        context: dict[str, Any]\n",
    "    \n",
    "    # Set up Redis connection for checkpointer\n",
    "    REDIS_URI = \"redis://redis:6379\"\n",
    "    checkpointer = None\n",
    "    with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "        cp.setup()\n",
    "        checkpointer = cp\n",
    "    \n",
    "    graph = create_react_agent(\n",
    "        model,\n",
    "        tools,\n",
    "        # highlight-next-line\n",
    "        pre_model_hook=summarization_node,\n",
    "        # highlight-next-line\n",
    "        state_schema=State,\n",
    "        checkpointer=checkpointer,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "!!! Important\n",
    "    \n",
    "    * To **keep the original message history unmodified** in the graph state and pass the updated history **only as the input to the LLM**, return updated messages under `llm_input_messages` key\n",
    "    * To **overwrite the original message history** in the graph state with the updated history, return updated messages under `messages` key\n",
    "    \n",
    "    To overwrite the `messages` key, you need to do the following:\n",
    "\n",
    "    ```python\n",
    "    from langchain_core.messages import RemoveMessage\n",
    "    from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "    def pre_model_hook(state):\n",
    "        updated_messages = ...\n",
    "        return {\n",
    "            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES), *updated_messages]\n",
    "            ...\n",
    "        }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be3889f-3c17-4fa1-bd2b-84114a2c7247",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Clear Redis to remove old checkpoints created with buggy serializer\n",
    "import redis\n",
    "r = redis.Redis(host=\"redis\", port=6379, decode_responses=False)\n",
    "r.flushdb()\n",
    "print(\"Redis flushed - ready for fresh execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213e11a-5c62-4ddb-a707-490d91add383",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-openai \"httpx>=0.24.0,<1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1885c-04ab-4750-aefa-105891fddf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        value = getpass.getpass(f\"{var}: \")\n",
    "        if value.strip():\n",
    "            os.environ[var] = value\n",
    "\n",
    "\n",
    "# Try to set OpenAI API key\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a00ce9",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0f089-070c-4cd4-87e0-6c51f2477b82",
   "metadata": {},
   "source": [
    "## Keep the original message history unmodified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6cbd3a-8632-47ae-9ec5-eec8d7b05cae",
   "metadata": {},
   "source": [
    "Let's build a ReAct agent with a step that manages the conversation history: when the length of the history exceeds a specified number of tokens, we will call [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) utility that that will reduce the history while satisfying LLM provider constraints.\n",
    "\n",
    "There are two ways that the updated message history can be applied inside ReAct agent:\n",
    "\n",
    "  * [**Keep the original message history unmodified**](#keep-the-original-message-history-unmodified) in the graph state and pass the updated history **only as the input to the LLM**\n",
    "  * [**Overwrite the original message history**](#overwrite-the-original-message-history) in the graph state with the updated history\n",
    "\n",
    "Let's start by implementing the first one. We'll need to first define model and tools for our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad19ee-e174-4c6c-b2b8-3530d7acea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if any([city in location.lower() for city in [\"nyc\", \"new york city\"]]):\n",
    "        return \"It might be cloudy in nyc, with a chance of rain and temperatures up to 80 degrees.\"\n",
    "    elif any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n",
    "        return \"It's always sunny in sf\"\n",
    "    else:\n",
    "        return f\"I am not sure what the weather is in {location}\"\n",
    "\n",
    "\n",
    "tools = [get_weather]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52402333-61ab-47d3-8549-6a70f6f1cf36",
   "metadata": {},
   "source": [
    "Now let's implement `pre_model_hook` — a function that will be added as a new node and called every time **before** the node that calls the LLM (the `agent` node).\n",
    "\n",
    "Our implementation will wrap the `trim_messages` call and return the trimmed messages under `llm_input_messages`. This will **keep the original message history unmodified** in the graph state and pass the updated history **only as the input to the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507eb58-6e02-4ac6-b48b-ea4defdc11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "\n",
    "# highlight-next-line\n",
    "from langchain_core.messages.utils import (\n",
    "    # highlight-next-line\n",
    "    trim_messages,\n",
    "    # highlight-next-line\n",
    "    count_tokens_approximately,\n",
    "    # highlight-next-line\n",
    ")\n",
    "\n",
    "\n",
    "# This function will be added as a new node in ReAct agent graph\n",
    "# that will run every time before the node that calls the LLM.\n",
    "# The messages returned by this function will be the input to the LLM.\n",
    "def pre_model_hook(state):\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=384,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    # highlight-next-line\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "\n",
    "# Set up Redis connection for checkpointer\n",
    "REDIS_URI = \"redis://redis:6379\"\n",
    "checkpointer = None\n",
    "with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "    cp.setup()\n",
    "    checkpointer = cp\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    # highlight-next-line\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8182ab45-86b3-4d6f-b75e-58862a14fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e8e76-5d43-44cd-bf01-a39212cedd8d",
   "metadata": {},
   "source": [
    "We'll also define a utility to render the agent outputs nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16636975-5f2d-4dc7-ab8e-d0bea0830a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream, output_messages_key=\"llm_input_messages\"):\n",
    "    for chunk in stream:\n",
    "        for node, update in chunk.items():\n",
    "            print(f\"Update from node: {node}\")\n",
    "            messages_key = (\n",
    "                output_messages_key if node == \"pre_model_hook\" else \"messages\"\n",
    "            )\n",
    "            for message in update[messages_key]:\n",
    "                if isinstance(message, tuple):\n",
    "                    print(message)\n",
    "                else:\n",
    "                    message.pretty_print()\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84448d29-b323-4833-80fc-4fff2f5a0950",
   "metadata": {},
   "source": [
    "Now let's run the agent with a few different queries to reach the specified max tokens limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffff6c3-a4f5-47c9-b51d-97caaee85cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\n",
    "result = graph.invoke(inputs, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb186da-b55d-4cb8-a237-e9e157ab0458",
   "metadata": {},
   "source": [
    "Let's see how many tokens we have in the message history so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba0253-5199-4d29-82ae-258cbbebddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = result[\"messages\"]\n",
    "count_tokens_approximately(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "812987ac-66ba-4122-8281-469cbdced7c7",
   "metadata": {},
   "source": [
    "You can see that we are close to the `max_tokens` threshold, so on the next invocation we should see `pre_model_hook` kick-in and trim the message history. Let's run it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c53429-90ba-4d0b-abb9-423d9120ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\n",
    "print_stream(graph.stream(inputs, config=config, stream_mode=\"updates\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe0399-4e7d-4482-a4cb-5301311932d0",
   "metadata": {},
   "source": [
    "You can see that the `pre_model_hook` node now only returned the last 3 messages, as expected. However, the existing message history is untouched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfc310-8f9e-4aa0-9e58-17e71551639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_messages = graph.get_state(config).values[\"messages\"]\n",
    "assert [(m.type, m.content) for m in updated_messages[: len(messages)]] == [\n",
    "    (m.type, m.content) for m in messages\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035864e3-0083-4dea-bf85-3a702fa5303f",
   "metadata": {},
   "source": [
    "## Overwrite the original message history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a4fd5-a2ba-4eca-91a9-d294f4f2d884",
   "metadata": {},
   "source": [
    "Let's now change the `pre_model_hook` to **overwrite** the message history in the graph state. To do this, we’ll return the updated messages under `messages` key. We’ll also include a special `RemoveMessage(REMOVE_ALL_MESSAGES)` object, which tells `create_react_agent` to remove previous messages from the graph state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2a65b-685a-4750-baa6-2d61efe76b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "\n",
    "def pre_model_hook(state):\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=384,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    # NOTE that we're now returning the messages under the `messages` key\n",
    "    # We also remove the existing messages in the history to ensure we're overwriting the history\n",
    "    # highlight-next-line\n",
    "    return {\"messages\": [RemoveMessage(REMOVE_ALL_MESSAGES)] + trimmed_messages}\n",
    "\n",
    "\n",
    "# Set up Redis connection for checkpointer\n",
    "REDIS_URI = \"redis://redis:6379\"\n",
    "checkpointer = None\n",
    "with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "    cp.setup()\n",
    "    checkpointer = cp\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    # highlight-next-line\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd061682-231c-4487-9c2f-a6820dfbcab7",
   "metadata": {},
   "source": [
    "Now let's run the agent with the same queries as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831be36a-78a1-4885-9a03-8d085dfd7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "messages = result[\"messages\"]\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\n",
    "print_stream(\n",
    "    graph.stream(inputs, config=config, stream_mode=\"updates\"),\n",
    "    output_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a0604-3d2b-48ff-9eaf-d16ea351fb30",
   "metadata": {},
   "source": [
    "You can see that the `pre_model_hook` node returned the last 3 messages again. However, this time, the message history is modified in the graph state as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f72f8-f817-472d-a193-e01509a86132",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_messages = graph.get_state(config).values[\"messages\"]\n",
    "assert (\n",
    "    # First 2 messages in the new history are the same as last 2 messages in the old\n",
    "        [(m.type, m.content) for m in updated_messages[:2]]\n",
    "        == [(m.type, m.content) for m in messages[-2:]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee186d6d-4d07-404f-b236-f662db62339d",
   "metadata": {},
   "source": [
    "## Summarizing message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langmem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e53e0f-9a1e-4188-8435-c23ad8148b4f",
   "metadata": {},
   "source": [
    "Finally, let's apply a different strategy for managing message history — summarization. Just as with trimming, you can choose to keep original message history unmodified or overwrite it. The example below will only show the former.\n",
    "\n",
    "We will use the [`SummarizationNode`](https://langchain-ai.github.io/langmem/guides/summarization/#using-summarizationnode) from the prebuilt `langmem` library. Once the message history reaches the token limit, the summarization node will summarize earlier messages to make sure they fit into `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9540c1c-2eba-42da-ba4e-478521161a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight-next-line\n",
    "from langmem.short_term import SummarizationNode\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "from typing import Any\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "summarization_node = SummarizationNode(\n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=summarization_model,\n",
    "    max_tokens=384,\n",
    "    max_summary_tokens=128,\n",
    "    output_messages_key=\"llm_input_messages\",\n",
    ")\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    # NOTE: we're adding this key to keep track of previous summary information\n",
    "    # to make sure we're not summarizing on every LLM call\n",
    "    # highlight-next-line\n",
    "    context: dict[str, Any]\n",
    "\n",
    "\n",
    "# Set up Redis connection for checkpointer\n",
    "REDIS_URI = \"redis://redis:6379\"\n",
    "checkpointer = None\n",
    "with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "    cp.setup()\n",
    "    checkpointer = cp\n",
    "\n",
    "graph = create_react_agent(\n",
    "    # limit the output size to ensure consistent behavior\n",
    "    model.bind(max_tokens=256),\n",
    "    tools,\n",
    "    # highlight-next-line\n",
    "    pre_model_hook=summarization_node,\n",
    "    # highlight-next-line\n",
    "    state_schema=State,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccaaca-5d9c-4faf-b997-d4b8e84b59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\n",
    "print_stream(graph.stream(inputs, config=config, stream_mode=\"updates\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caaf2f7-281a-4421-bf98-c745d950c56f",
   "metadata": {},
   "source": [
    "You can see that the earlier messages have now been replaced with the summary of the earlier conversation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}