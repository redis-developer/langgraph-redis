{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992c4695-ec4f-428d-bd05-fb3b5fbd70f4",
   "metadata": {},
   "source": [
    "# How to manage conversation history in a ReAct Agent with Redis\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - [Prebuilt create_react_agent](../create-react-agent)\n",
    "    - [Persistence](../../concepts/persistence)\n",
    "    - [Short-term Memory](../../concepts/memory/#short-term-memory)\n",
    "    - [Trimming Messages](https://python.langchain.com/docs/how_to/trim_messages/)\n",
    "\n",
    "Message history can grow quickly and exceed LLM context window size, whether you're building chatbots with many conversation turns or agentic systems with numerous tool calls. There are several strategies for managing the message history:\n",
    "\n",
    "* [message trimming](#keep-the-original-message-history-unmodified) - remove first or last N messages in the history\n",
    "* [summarization](#summarizing-message-history) - summarize earlier messages in the history and replace them with a summary\n",
    "* custom strategies (e.g., message filtering, etc.)\n",
    "\n",
    "To manage message history in `create_react_agent`, you need to define a `pre_model_hook` function or [runnable](https://python.langchain.com/docs/concepts/runnables/) that takes graph state an returns a state update:\n",
    "\n",
    "\n",
    "* Trimming example:\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langchain_core.messages.utils import (\n",
    "        # highlight-next-line\n",
    "        trim_messages, \n",
    "        # highlight-next-line\n",
    "        count_tokens_approximately\n",
    "    # highlight-next-line\n",
    "    )\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    from langgraph.checkpoint.redis import RedisSaver\n",
    "    \n",
    "    # This function will be called every time before the node that calls LLM\n",
    "    def pre_model_hook(state):\n",
    "        trimmed_messages = trim_messages(\n",
    "            state[\"messages\"],\n",
    "            strategy=\"last\",\n",
    "            token_counter=count_tokens_approximately,\n",
    "            max_tokens=384,\n",
    "            start_on=\"human\",\n",
    "            end_on=(\"human\", \"tool\"),\n",
    "        )\n",
    "        # You can return updated messages either under `llm_input_messages` or \n",
    "        # `messages` key (see the note below)\n",
    "        # highlight-next-line\n",
    "        return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "    # Set up Redis connection for checkpointer\n",
    "    REDIS_URI = \"redis://redis:6379\"\n",
    "    checkpointer = None\n",
    "    with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "        cp.setup()\n",
    "        checkpointer = cp\n",
    "        \n",
    "    agent = create_react_agent(\n",
    "        model,\n",
    "        tools,\n",
    "        # highlight-next-line\n",
    "        pre_model_hook=pre_model_hook,\n",
    "        checkpointer=checkpointer,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "* Summarization example:\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langmem.short_term import SummarizationNode\n",
    "    from langchain_core.messages.utils import count_tokens_approximately\n",
    "    from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "    from langgraph.checkpoint.redis import RedisSaver\n",
    "    from typing import Any\n",
    "    \n",
    "    model = ChatOpenAI(model=\"gpt-4o\")\n",
    "    \n",
    "    summarization_node = SummarizationNode(\n",
    "        token_counter=count_tokens_approximately,\n",
    "        model=model,\n",
    "        max_tokens=384,\n",
    "        max_summary_tokens=128,\n",
    "        output_messages_key=\"llm_input_messages\",\n",
    "    )\n",
    "\n",
    "    class State(AgentState):\n",
    "        # NOTE: we're adding this key to keep track of previous summary information\n",
    "        # to make sure we're not summarizing on every LLM call\n",
    "        # highlight-next-line\n",
    "        context: dict[str, Any]\n",
    "    \n",
    "    # Set up Redis connection for checkpointer\n",
    "    REDIS_URI = \"redis://redis:6379\"\n",
    "    checkpointer = None\n",
    "    with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "        cp.setup()\n",
    "        checkpointer = cp\n",
    "    \n",
    "    graph = create_react_agent(\n",
    "        model,\n",
    "        tools,\n",
    "        # highlight-next-line\n",
    "        pre_model_hook=summarization_node,\n",
    "        # highlight-next-line\n",
    "        state_schema=State,\n",
    "        checkpointer=checkpointer,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "!!! Important\n",
    "    \n",
    "    * To **keep the original message history unmodified** in the graph state and pass the updated history **only as the input to the LLM**, return updated messages under `llm_input_messages` key\n",
    "    * To **overwrite the original message history** in the graph state with the updated history, return updated messages under `messages` key\n",
    "    \n",
    "    To overwrite the `messages` key, you need to do the following:\n",
    "\n",
    "    ```python\n",
    "    from langchain_core.messages import RemoveMessage\n",
    "    from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "    def pre_model_hook(state):\n",
    "        updated_messages = ...\n",
    "        return {\n",
    "            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES), *updated_messages]\n",
    "            ...\n",
    "        }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be3889f-3c17-4fa1-bd2b-84114a2c7247",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a213e11a-5c62-4ddb-a707-490d91add383",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-openai \"httpx>=0.24.0,<1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a1885c-04ab-4750-aefa-105891fddf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        value = getpass.getpass(f\"{var}: \")\n",
    "        if value.strip():\n",
    "            os.environ[var] = value\n",
    "\n",
    "\n",
    "# Try to set OpenAI API key\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a00ce9",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0f089-070c-4cd4-87e0-6c51f2477b82",
   "metadata": {},
   "source": [
    "## Keep the original message history unmodified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6cbd3a-8632-47ae-9ec5-eec8d7b05cae",
   "metadata": {},
   "source": [
    "Let's build a ReAct agent with a step that manages the conversation history: when the length of the history exceeds a specified number of tokens, we will call [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) utility that that will reduce the history while satisfying LLM provider constraints.\n",
    "\n",
    "There are two ways that the updated message history can be applied inside ReAct agent:\n",
    "\n",
    "  * [**Keep the original message history unmodified**](#keep-the-original-message-history-unmodified) in the graph state and pass the updated history **only as the input to the LLM**\n",
    "  * [**Overwrite the original message history**](#overwrite-the-original-message-history) in the graph state with the updated history\n",
    "\n",
    "Let's start by implementing the first one. We'll need to first define model and tools for our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaad19ee-e174-4c6c-b2b8-3530d7acea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if any([city in location.lower() for city in [\"nyc\", \"new york city\"]]):\n",
    "        return \"It might be cloudy in nyc, with a chance of rain and temperatures up to 80 degrees.\"\n",
    "    elif any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n",
    "        return \"It's always sunny in sf\"\n",
    "    else:\n",
    "        return f\"I am not sure what the weather is in {location}\"\n",
    "\n",
    "\n",
    "tools = [get_weather]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52402333-61ab-47d3-8549-6a70f6f1cf36",
   "metadata": {},
   "source": [
    "Now let's implement `pre_model_hook` — a function that will be added as a new node and called every time **before** the node that calls the LLM (the `agent` node).\n",
    "\n",
    "Our implementation will wrap the `trim_messages` call and return the trimmed messages under `llm_input_messages`. This will **keep the original message history unmodified** in the graph state and pass the updated history **only as the input to the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b507eb58-6e02-4ac6-b48b-ea4defdc11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "\n",
    "# highlight-next-line\n",
    "from langchain_core.messages.utils import (\n",
    "    # highlight-next-line\n",
    "    trim_messages,\n",
    "    # highlight-next-line\n",
    "    count_tokens_approximately,\n",
    "    # highlight-next-line\n",
    ")\n",
    "\n",
    "\n",
    "# This function will be added as a new node in ReAct agent graph\n",
    "# that will run every time before the node that calls the LLM.\n",
    "# The messages returned by this function will be the input to the LLM.\n",
    "def pre_model_hook(state):\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=384,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    # highlight-next-line\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "\n",
    "# Set up Redis connection for checkpointer\n",
    "REDIS_URI = \"redis://redis:6379\"\n",
    "checkpointer = None\n",
    "with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "    cp.setup()\n",
    "    checkpointer = cp\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    # highlight-next-line\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8182ab45-86b3-4d6f-b75e-58862a14fa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAFcCAIAAAAlFOfAAAAQAElEQVR4nOydB1xT1xfHbzYZ7LCHgCDbhRMH7oniFmutVm3rrNattVatVmu17tZVtdY666qjTlRU3ANFEWXL3iMDsvgfzL8ptohaM+5L7veTT3zv3ZfnI++Xc849dzGrqqoQgWBomIhAwAAiRAIWECESsIAIkYAFRIgELCBCJGCBEQqxUqoszJJJypWScoVCUaWQUSA/xeHSmWwaz5zJs2A4uJkh08N4hCgukz+/J06OE5UVys1tWDxzBjxXCxsWokKiVKVEuamVknIxi0NPfyrxDOJ7BcNLgEwGmhEktFXKqpjjhQVZlbbObK8ggYs3F1GZCokyJU6c8VySlVwRGm7r08QcmQCUF+LjG6WXDuaH9rFt0sEaGRdg2mNOFFZKlN1GOHIFDGTUUFuIlw7mmfHorXoLkfFSkF15dGNmj5GOrj48ZLxQWIjnduc6epoFt7FEJsCRjZnt+guFzhxkpFBViEd/zPRuLAgKNQkVqjmyMSO4jRX81cgYoSMKcuVovkcA36RUCPSf6Hrjz8LiXBkyRqgnxIR75UwWvXEHK2R6DJ/jfvFgnlH23KOeEC8fzG/ayRRVCNBoNHAFkKtCRgfFhHj3fHFQGwsO18hzGXXQtJP1k5tlFWIlMi6oJERwSekJktBwY07WvA3tB9g9uFyCjAsqCTH5kRjaZJHJ4+7Li4spRcYFlZ4rNHxBIyzSL7Nnzz5+/Dh6d7p06ZKVlYV0ALSyWAnZ2alSZERQSYgl+XKvYH0LMT4+Hr07OTk5JSU69J4NmglePJMgI4IyQoTwvDhPprtqytGjR4cMGdKmTZvOnTvPnDkzNzcXDjZr1gys2qJFizp06AC7SqVy06ZN/fr1Cw0N7dmz5/Lly6XS/5slsH979uz5/PPPW7dufeXKlfDwcDjYt2/f6dOnIx3At2AWZBhVQpEyQhSXKeDbR7rh/v37S5YsGTZs2P79+9euXQvGbM6cOXD81KlT8A66PHbsGGyA1Hbu3DlhwoR9+/Z9/fXXly9f3rhxo/oKTCbz8OHD3t7emzdvbt68+bJly+Dg7t27Fy9ejHQAfBXwhSAjgjL9EcVlSr6FrsxhUlISh8Pp06cP6MnV1RVMXXZ2Nhy3tKxuvOHxeOoNsIJg8EBtsO3u7t6tW7dr166prwAZPjMzM7CI6l0+vzqEsLCwUG9oHb4lQ1xqVBkcygixSlXF1lmVGVwwKGns2LEREREtW7Z0dna2tbX992lWVlYnT54E25mXl6dQKCQSCWhUU9qwYUOkLxhMGtvMqBIIlPljeBbM0nw50g0eHh47duwAW7h+/XoI7EaNGhUXF/fv077//vtt27ZBKLl161Zw0/37969ZKhDorzuCqEQBWkRGBGWECH4ZvDPSGT4+PmDqzp07B0Eeg8GYOnWqTPZKbQBqKhApjhw5slevXi4uLkKhUCQSIQOh00DFIFDHIpozbRxZKpVO2vvB/j18+BA2QIIhISHjx4+H+kph4f+bdNWdDFQqFWhRHSwCYrE4Ojq67v4HuuudUClR2rkZVd9EKsUZZjwGNK4gHRATEzNt2rQLFy5kZGQkJCRApdjJycnR0ZHzknv37sFBCCJ9fX1PnDgB5zx//hxMJuR6ysrKUlNTIV78xwWhmgLvV69eTU5ORjog4W65kwe1h+b8AyoJ0SOQn/pYJ0IcPXo0BHxr1qwZNGjQxIkTwZKtW7cOlAdFEC+eP38eUjaQMlywYAEYRYgR586dGxkZCWeCWD/66COou/zjgv7+/pBrXL169YoVK5C2USqqMhOl7n5GNXKASj20pSLF2d25EeNckGmT8lj04pm0fX87ZERQySJyBUxrB3as0XU8eVdi/ig0vt7pFBtg36aPcPOcpEZhtXeMBb8JDXS1FkEVmM1m11rk6ekJuRukG3a+pNYiSPe8rt4Nnv2nn36qtejpnTJ7NzMbh9r/FupCvcFTDy6X0GhVjdrXPoq5vLy81uOVlZUgRHXY9w/odLqO2j/U/+8/0kAa5HI5i8WqtQgq7zVT5TU5sS0rbJCduVXtH6QulBzFBw8jsJWl/ruEGRwj/sMp2UwUPtY5+nB+YU4lMiWi9uc5epgZ68+PquOaoel5/6oX7QfYOdc3qnTa67h4IM/Vh2vE8+BQteGcRqdFznS/fqow/lYZMmpUyqojGzNtHNnGPRsT5SdhijlRkB4vCe0jNLIEr5rbZ4sS7pR3GGxn3BPfIOOYli4/szLmeAHfggluGkIoLp/yvQHyXlSkJ0junC1u3MGqRQ8bOt2oOtrUijEIUU3GcwkYj5Q4sZ0bx1LIAl3Ci2fBUKkQ/jBoqLRILi5VVqGqp7fL4c69G/EbtrdisU1l1KLxCFFDdoq0IFMmLlPAi06jSUTa7DwmkUjS0tIg4Yy0irk1Cx4E35JhbsNyrc/lW5rc3OZGKESdEh8fv3Tp0t27dyOCViGrChCwgAiRgAVEiAQsIEIkYAERIgELiBAJWECESMACIkQCFhAhErCACJGABUSIBCwgQiRgAREiAQuIEAlYQIRIwAIiRAIWECESsIAIkYAFRIgELCBCJGABESIBC4gQCVhAhEjAAiLEd4NGo9nZGdXk1ZhAhPhuVFVV5efnI4K2IUIkYAERIgELiBAJWECESMACIkQCFhAhErCACJGABUSIBCwgQiRgAREiAQuIEAlYQIRIwAIiRAIWECESsIAIkYAFZMGft2LYsGEikYhGo8lkstLSUqFQCNuVlZVnzpxBBG1gKku9vSc9e/bMy8vLysoqKCiQy+XZ2dmwbW5uzOvW6hkixLciMjLSzc2t5hGwiGFhYYigJYgQ3wo2m92vXz8G4+8FeN3d3QcNGoQIWoII8W0ZMmSIi4uLehvMYceOHZ2cnBBBSxAhvi1gFAcOHKg2imAOBw8ejAjagwjxHQCj6OzsrDaHDg4OiKA9dJtHlJQrCrNlcpnxZIgiun566dKltk0HJseJkVFAQ4hvybBxYDPZhrRKusojSsXKqH152SkV7n78CrE215AnaBcmm1ZaIFfIVA1CzFv2sEEGQidCBEN4ZENWaD97obMZIlCEu+cK6AzUvr8QGQKdWOPflqd3G+VCVEgtQroKq6poMScKkSHQvhDvRxUHt7M24zEQgWo07WyblSwVlSmQ3tG+ELPTKviWLESgJnQ6rShbhvSO9mvNSnmVhTURIlWxcTQrK5IjvaN9IUrKlSrSoYeyyCtVSIX0D+mPSMACIkQCFhAhErCACJGABUSIBCwgQiRgAREiAQuIEAlYQIRIwAIiRAIWECESsICMWfkvrF333cdjhtR9TnJyYsfOzR49elD3aUu+nT95yhikPSL6d9716zZENYhFJGABESIBCwwvxGfPn3427sNvFq08dHjv88SnDAazR/c+n336OZ1OP3L0wK5ft86YNn/lD0u6de09ftxUhUKx+7efoy6ezc3NtrNzGDxoeETfN0y3kJaWMmr04BXfbdi7d+ez5/F8vuCTsZOdnV3Xr1+R/iLVycll+rT5/n6BcKZMJvt5+48XL50tLi6ytRV26dxz1MjPmMzqr6igIP/7Vd88eHAHPt63z8Ca1y8pKf5x0+rY2LulpSVeXj6fjJ3UpHEz9C4wGIwrVy9u2bo+JyfLza3erJlf+/kG1H0/dRTV5MGDuzNnT1zyzQ8tW4QivDG8EJmM6nvYvHXd3DmL4QHcuHF1wcKZ7u4evXv1Y7FYFRXSw0f2zZ61EI7AaZs2rz156sjUz+cEBjW6e/fmho0r4duHM+u4PuPl49m+46e5sxe5uLgt/+7r1Wu+DQxo+M3iVRYWlnPmfr5+w/c/btgJ56xZu/zqtUtTp8zx9Q148uTRmrXLKisrJ06YBkXLli/IyExf9u1aWxvh0WMHoq9EwWfhuEqlmj1nskgsgjuEomN/HIQL/rRxl5eXN3pr8nJzjh8/NGvGgup7WLcc/q9fdvxe9/3UUaQhIyMdvsnIoR/hr0KET2Wla5deAf5BYAVDQ9uDRTlz9gR6ObNHRUXFoIEftGrZxtnJRSQSwZMeOmRE9+7hri5uYAu7dwvfs3fn21y/Y4euIGWwPR3Cukokkl69+gmFdmw2u337zklJz+AEsGdnz538aMTYTh27uTi7du3Sc0D/yBMnD8vl8vz8vHv3bw+LHNW0SfN69Tw/nzyLx+OrL3vn7k2w6DOmz1cXTZo4w8HBCX456F0oKi78ct6S4ODG8IL/ND09Ff7SOu6njiLNNeGcOfOmtG7dbszoCYgK4CLEBj5+mu169byysjI0uwEBweoNUAy45mYhrTRFjRqFwJkgrDde393NQ73B4/Nr7vJ5fNlLkpKfK5XKAP9gzUfA2MDPAOxKWnoK7Pq9dN/o5c9Dsx0fHwdmu3GjEPUu/JAaBjdJTExA74Kbaz1LSyv1trVV9chiqVRSx/3UUaTeVSoVYAvt7RxmTv8KUQRcKitcLq/GNlckKtfsQlim3pBIqidX+GL6ZyAF9RH1oGywKDwer+7rM1mvDKNhczg1d+E66otrTJ3mlkAT8IINDvvvj/D+ulv4FNih7j3/9n0gERsbW/QumHG5mm31n1b3/dRRpN6FaBtO8vDwgpv5d+CIJ7jcpeZLBMQSsUBQyxyYakWCF/PyfCUCg58+em/UF1c/YzXqbTgufrkhFos0RZrfCZSCf9+6eU/NS4FdRLq8n0pZ5euK1Lvu7p5fTJ37xbRPt2xbP3niDEQFcHHND2LvarYTEp5oXGdNoE4KfhDqiRDtqV9QYwCnBlJA7w1cHCLIuMexmiOPHz8UCARQvwHXCbuJL0NJAMIDzd2Cjwa3DoZHc0tsNkcotEfvTR33U0eRerdVy7Y+3r6TJ848fHjf7Ts3EBXAxSLGXI/28fHz9w+6du0SVAPnzVn873Pguw4PH7Dzl80gPlAAZHA2/rgKkjjLlq5B742lhWXPHn1/27PD2ckV7gQyNeqKEbg2R0cniFP37N0BT9rKyvrQob2svxx9SNMW8Mi/XfbVxAnTHRydQBDr1n03fPho+CB6P+q4nzqKal4BqnTXb1z5bsXCX3Yc4vP5CG9wEeLoj8dDTXnlqm/AosB21669aj1twrgvzAXmW7auKywsgFAstHX7MaMnIi2hrg5DAgVSg+DuPxw+5oNho9RF879cunLlN1/O/0KdR4Q6PmRw0MsU4HfL1/+0ec3Xi2ZBpsnR0XnEiLGQ3UTaoI77qaOoJuCgx3wSeeTo/g+Hj0Z4o/1JmPavetGil73QmfOW50ObLHxZ69Zsg+QFIhiaGyfynTzYQW0skX4hTXwELDAGIUJOe+++nbUWQf1x4/odyKD0iejwuqI5sxa1aUOWJqjG8K75/SkXldfMO9aExWRBCwoyKNk5Wa8rgvS1mRlek/cR1/zfgeqLuQDftXecHJ0R4U2QGJGABUSIBCwgQiRgAREiAQuIEAlYQIRIwAIiRAIWECESsIAIkYAF2heitT0LVZFlBagKDEK+fQAAEABJREFUm0tnmRmgu7T2/0u2Gb0gqxIRqEnGc7GtoxZ6vL8r2hdivQBecS4RIiWpkCi5fIbQRX8dVjRoX4iegQKOGe3O2QJEoBrnd2e17WeY1Ul1tV7z1WMFUpHKzp0rdDFjMmmIgC9VohJFWaHs1p8FQ6e7WTsYwC8j3QkRSHokSnogqpRWFWZr01OrVCq4Z9VfVL0E/8FBr6OiosKwXRI5UDvh0J29zJp3s4ENZCB0KERdEBERAU+usrJSJKoeZQw3T6PRnJ2dly5dGhwcjCiIVCoNCwuLjo7GrYesnqHYRJ2ZmZmFhYVqFaK/5kUICQmhqArRy2ktrl+/3rlz54ICk46qKSbEO3fu/GMeBVdX108++QRRGQaDce3ateHDh6enpyNThXpTF8+aNUuzzWQywZaAa0bU58yZM1OmTImPj0cmCZWECNHhhAkTnj9/bmf3//FQTk5OY8eORcbCkSNHINgFq49MD8oI8c8//wTjN3LkyLlz58I2i8Xi8XjDhg3j1phKywjYvXv31q1bL1++jEwMatSawR2z2ewlS5bUPDhw4MBDhw4hY2TatGnwq+vduzcyGXAXIuQ1ZsyYsWzZMngwyJRYsGABpAIGDx6MTAOshbh48eLi4uKVK1dCvRKZHsuXL3d0dBw1ahQyATCNEe/du9exY8dGjRqtXr3aNFUIzJkzp7y8fMOGDcgEwNEirlq16unTp/BuYWGBTJ4dO3bk5eXNnj0bGTV4WcSEhITw8HBIykDNkahQzccff+zp6QkhIzJqMLKImzZtgqoJGEIQIiK8ysmTJ6OiouDLQUYKFhYxIyMjMjISYsE9e/YQFdYKpHL69Okzbtw4ZKQY3iJCCvfgwYNQNfbx8UGEOrl9+/a6det+/fVXZHQY0iKWlJSMGTMmPz//2LFjRIVvQ/PmzaFhacCAAcjoMJhFPHr06Pr16yHoadyYTJ39bqSlpX366adnzpxBRoQBhCiXy6dPn25nZ/fVV5RZoAs3CgoK+vXrB3U7rSwuhAP6/jPOnTvXrl27oUOHEhW+D0KhEL7Jli1bQsYbGQV6nelh3rx5KpXqxg1qrIWEOVwuF+ouHTp02LdvH7QEIoqjJ4sYExMTGhoaFhYG7aeIoD0uXboEFb6kpCREcfQRIy5dujQnJwcSNByOAUZumwJDhgyZP39+w4YNEWXRrUWMjY3t2rWrv78/VJCJCnXHgQMHVq9eff36dURZdBgjrl27FoS4f/9+GxsbRNAxO3bsmDRpkpmZWZMmTRAF0ZVF3Lu3eu3q7du3ExXqjQ0bNmzZsgXaSxEF0ZUQExMTwSMjgn6RSqXFxcWIgujKNauHvhMIbwmZMZaABUSIBCwgQiRgAREiAQuIEAlYQIRIwAIiRAIWECESsIAIkYAFRIgELCBCJGABESIBC4gQCVhAhEjAAiJEY2DQoEFMJpPNZicnJy9YsIDD4cA2HNm+fTuiCESIxoBEIsnLy1Nvv3jxAr1ck2vYsGGIOhjJPAEmTrNmzZRKZc0jLi4uH374IaIORIjGwMiRI0F5NY+EhYVRa9Q9EaIxUL9+/ZCQEM2uk5PT8OHDEaUgQjQSPvroIwcHB/QyOuzUqRPlJiEhQjQSwChCpAgqdHZ2/uCDDxDVILVmAyAuU6iUSOsMGTDy3q34Tu078di25cUKpFVA4hY2LKQziBD1yrU/8p/eFtk4sUvz5Uj70Ae2WoEk6NA67Y+xt7JjZyZJvIL4zbra2Llqf/YYIkQ9oVRWHVyd4dfCss84a66Akl+7SlVVWiA7szun01AHZ08zpFVIjKgnDv6Q0bSzTf1GFhRVIUCn06ztORHj6106mJedIkVahQhRHzy6WlovSODkxUdGQecPnO+c0/LEJkSI+iAzWcozN54oCP6WnNSKCrE2K1xEiPqgSoXAqSEjwt1fUJQnQ9qDVFb0AcT4lFig/e0pK5TRqrQ5zxYRIgELiBAJWECESMACIkQCFhAhErCACJGABUSIBCwgQiRgAREiAQuIEAlYQIRIwAIiRAIWkN43pk5KSlLkB+HI0BCLaOo8exaPMIBYREx5mvBkxswJEf079+zddvyEj+7cvakpOn7iMNiw7j1Dv5j2WXp6asfOzS5eOqcuevb86azZk+BTvfu0/2rBjJycbPXxY3/83m9Al/j4uPETR4b3DftgeN9Tfx6D4zt/2bx8xcLc3By4yLVrl5HhIELEkcrKytlzJrPY7JXf//jTxl0BgQ2/WjA9P796mqX4p49/WP1taGjY1s17evbo+82SeeivJThBT9Omf0aj01ev2rxq5aay8tLpM8fLZNXdV5lMplgs2rV726KvVxw/dqlbt96r1yyDC0YOHTlgQKS9vcPRw+dbtAhFhoMIEUcYDAaIac6shT7evh4eXqNHja+oqIh7HAtFZ8+esLa2mTh+mru7B+ipXbtOmk/9cfx3UOT8L5d6eXn7+QbMm/NNdnbm5egL6lKFQvFB5CjQHJzTs0cE7CYlPTMzM+OwOXDE0tKKxdLhsOU3oqsYkU6nk5Vy/zNgwOQK+br1KxKTnolE5ere3WVlpfAOvjgwoCEoVX1mu7Ydd+zcpN4Gz+vnG2guMFfvOjg4Ojm5JCYmdO3SU33Ey8tHvWFubgHv5aJyhA26EqJKpTKyzvH6JCMjffqMcU0aN5839xuhrR18mUMie6mLQI62QjvNmRYWlpptcL7PExO69WitOSKXywuLCjS7HM6r42ZwekCk1owjURfPKpVKcLJq6UDwpymCwLGyokKzW15eptnm8wXBwY2nf/FlzUtxuTxEBYgQcUQul3E4ZhoDdu78KU2Rq6v7w4f3wNuoI58rVy9qivz9g86cPeHs7AqeXX3kxYs0W1shogKksoIj/n5BpaUlf57+o7Cw4Oixg08THltZWSdVx4uiDu27gIGEuDArO/P8hdMx16M1n+oTPlAqlXy3YiE4aHDuu37d9vGYIU+fPq77/xIIzOF/efjwfnFxETIcRIg4EhrafuiQEZu3rBs1elBc3IM5sxZF9B0E1m7bzxugaPTH4yGVOPaTyAtRp6d9UZ2+gZovvDs6Ov2wanNRUeHnU8aMmzDi1u2YJd/8EBAQXPf/1blTDzCikOi5e+8WMhw0HVUplixZEhQU1K9fP0RAaN/K9NZ9HGwctTDGHp4XSE3jcMGSTfnik+3b9nt61kd65PSOjLZ9hU5eWpuKiVhEihEbe2/QkB7gdsH5xsXF/vjTD35+gZBrRBSHVFYoRuPGIXNnL9p/8Nc9e3dAeNe4Uchnn04xgpQtESL1gAYVeCHjggiRgAVEiAQsIEIkYAERIgELiBAJWECESMACIkQCFhAhErCACJGABUSIBCwgQtQHVnYcIxvAYyFka/cvIr1v9AGdgYpyKpERkRpXbuPERtqDCFEfuHibiUu1vG6tASkvlrk24LHNtCkeIkR9ENjKMjdVmhRbhoyCc79mt+ppg7QKEaKe6D/JOfVx+dNbJSV5VPXRUrEiJ01y8IeUvp862TppeUU3UlnREzQard94lzvni45vT7BzsCnO1eZCdq9DqVJVz3SAtABEhCV5cs8g3uCprubW2p8TgghRr2w9OHfKlCkNvF2USn0Mbl+6dGmLFi26du2K3puqKmTG06H/JELUE48ePQoODl6zZg2Xy0XV37s+0jmDhkSUl5dzuBQIwIgQ9cH69eudnZ1BiGoV6o2GDRsiikAqK/oAVDhw4ECkdxQKxU8//YSoABGiDklKSvr5559hwyAqRC9nFYuKikpOTkbYQ1yzrgBrNHfu3D179iCDMn36dEoMNiVC1AlxcXHe3t4HDhxAhqZVq1aIChDXrH1GjBhhYWFhZqa16Tjeh4yMjJ07dyLsIULUJlKpND4+Hjyyu7s7wgN7e/vNmzcj7CGuWWscOnQoKCjI398f4QSbzV6xYoVIJBIIBAhjiEXUDmAIExISfH19EX60a9cOcxUiIsT3B2rHRUVFlpaW8+bNQ1gSExNz+PBhhDdEiO9Ffn5+mzZtoGoCKWuEK3B7x44dQ3hDYsT34vbt2zdv3kR4ExgYOH78eIQ3xCL+RxYvXgzvvXr1QtgDCW38s4lEiP+FLVu2tGzZElEHSCVevXoVYQwR4rsBTSboZdtx9+7dEXWAWvOVK1cQxpAY8R24cOFCdHQ0JAttbW0RpQgPD8/OzkYYQ4T4DkBaeNGiRYiCQHujp6cnwhjimt9MWVmZumoSERGBKMvMmTNTUlIQrhAhvpnJkydPnDgRURzIJsbGxiJcIa65Lq5fv966detffvkFUZ9p06ZBIxDCFWIRX8usWbNwfnLvCp/Ph3ZIhCtEiLUAlRJ47927d7t27ZAR0aVLF4QruhKijY0NJj1D35WEhAR17jcsLAwZF6GhoU+ePEFYoishCoVCnEPjOoAETY8ePZAxMm/ePD8/P4QluhJiQEAAtj++15GamqpUKg0+3El3gI+i0zENxnR1W/DLe/r0KaIOUVFR0HzHYDCQ8TJu3LjHjx8jLNGVEJlMppeX17NnzxBFuHHjBrSDIaOmoqJCpVIhLNHVwuHoZUepRo0a4d8ace3atTZt2iATAITIZrPx9M46vCdKhIn79++XSCTINDDFGBHw9/ePj49HeMNisbQyaxslMMUYEWFvEX/77Td4HzBgADIZcI4RdShEGo3m6+uLZ91569at3t7eyMTYtGlTYGAgwhLdRgzYeufmzZtTq6+/VjDRGBFh6Z0XLFgA740bN0amh4nGiAg/i7hu3bqPPvoImSommkdU06xZszt37iA8yM/Pt7OzQ6aKieYR1YBRxME79+/fH95NWYXIlGNEhId3XrNmjTpZY+KYboyIDF1fAV8M71OnTuXxeMjkMdE8ohoDWsTS0tIRI0Ygwl+Ybh4RgJx2YmKiUqlEuiciIkKTHVQoFJcvXz59+jQi/IVJx4hIX0bx7NmzhYWFoPjQ0NCbN2+KxeK+ffsiQg1MOkZE+hLiuXPnpFIpbMhkskmTJuE8Ys1QmHSMiPRSX8nOzk5ISNAsKALJ0Q4dOiDCq5h0jIj0YhGjo6Pz8vJqHhGJRMY3DO89MfUY0cfHJyUlRaeD1c+fP1/z+o6Ojh4eHqbT0fAtwTlG1NOUI2rvrKPFMsHcpqWlMZlM0B/86Nu1a9ekSZOmTZtSdGC17qBwW3N+ZuX9qJLc9Aqp6L3yL0qVCuI3Ok1XBlhJF0ErqoMHq/swTxabTF/xCiEhIfCUawbQsO3k5HTixAmEDXVZxNQn4pjjhQ3DbAJCrbkCrKdrotNppYWy8mLZtvkpw+e4W9iwEOEvXF1dMzMzNbugQg6HM2bMGIQTr7WIT2+XPblV3vVDF0Q1Dq9N7TvO2dqejQgv2bZtG9SXax7x8vLCYcHKmtTuxSokyic3KalCoMuHztf+KECEvxg2bBgYRc0uxDCRkZEIM2oXYnZyBYNJgUV+a8XClp2TWikpN54Z5d4TPp/fp08fzVheCLUAAApwSURBVCQWbm5uGA4Zq12IZYVyh3oU7q7iEcgvzJIhwl+ACVQvmArmcOjQoQg/ahdiZYVKIcO0nv82iEsVSoVue55TCzCK4eHhkM0GOeI5gpZMXYwj5SVySalSUq6UipXySu1YhEDXXs0b5LZq1So2ugRpA0iTMdk0njmDZ860cXzfqiERIkZA1jbxgTgxVsRkMyskCnhncrT5gNqGjEJy9OSudoIWJptRKZEpZUrITErL5e5+fN8Qfv2G/3E9XiJELCjOk136vaCigkZnsYT1hVwLDqIUSrmyLF8Sc6rs8uGC5t1sgkMt0DtChGh4LuwvSIkT23tbO3nyETVhsBjWzubwUsiUD2OK7pwt7jXa0cH9HX5OpDXMkKhUVTsXp4kkLO9QVwt7qqqwJuCvXQLtnIPsT+3IfXyj7O0/SIRoMOQy5Y8zkhz97S0d/2NchS0cPtuzhUvsVXHc9fK3/AgRomGQVah2LEwL6uppJjDapkjnQPuH10S3zxa/zclEiIbh12/TPZpRsgX1nQAtJtyXQB7gjWcSIRqA07vyHHyEbK5J1BRdGzreOltakv+GnBERor5JeSLOzZALhFxkMggcLCAzUPc5RIj65sqRQsjUIFPCwo4nKlFmJtU1VzkRol55dq/MzJLLNadYvvr9sfO2uXexrmwORkL8euGs6TPGI6Mm/rbYTICvCmPjLsz4qqVYrJ3G6JrwLM2yk6Xi0tf2zdOaEI8cPbB8xUJEqJMXCWJzexOdDsrcjpcc99rqs9aE+OwZ7itZGJzUJ2JbN4FmEJOpYW7HT0+oeF2pdjIIU6d9Ght7DzbOnDmxZfNvPt6+jx492PrzBlAnfO/+fkGffDLZ3+//cwycPHX0wMHdWVkZXC6vZYvQ8eO+sLGx/ccF4ZzfD+3Jzs7kcMwaNWw6aeIMe3sHRHEKc2SIpsNY6P7Ds5ev7cnNT+FweE2Cu/XsMp7Nrh5Qu2vfPBC/r0/ri9G7Ssvz7YX1+ofPqOcWDEVKpeLYqdX3Hp6uUqkCfNt6ezVDOoPNY7149lohaud7WbL4hwY+fp06djt6+LyXp/eLF2kzZk2wE9pvXL9zw7odXB5vxszxeXm5qHqqpJMrVy3p1rX39m37Fy/8/tnzp3PnTfnHAK6HD+/DOQMHDPt52/5l364tLStZ9M0cRH2g5sji6Cp3GPfk8m8Hv2rg3WL6xN1D+3/18HHU738sUxcxGMyUtNj0F4+nTti1cPZpHs9y/+El6qKo6F9u3jnat+fULybs8vRofP7ydqQzWBxGhVjHMaJAIGAwmSw229LSisFgHPvjd7B2c+csrl/fB15fzl2iUCjOnK0eRXvw99/atAkb/sHHbm71GjcOmTxpJmgxLu6VlZ1TUpM4HE6P7n1cnF0D/IO+/mr5xAnTEfWBUJ3J0dXqp1FXdnl5NO3VdYLQ1s2/QWjvbhPvxZ4uKc1Vl8pkUlAbh80FG9m0YY+8glSZrNo43Y39MyggrEXTPvCp0BYDG9TX4ZIfNDqNyaJLxbUPkNeJp3j2PB4MJJP5/18/j8cD2SUlPQM5JiU/D/AP1pzp6xsA74lJryxi2qRxM3Don08de+LkkeycLHDcIEdEfeBJ0Bg6CRBVKlVGVjyYQ80RECW8Z+ckqndBZ2o3DfC41Z0FJdIyhUJeUPjCzSVA8yl3V91O0cQ1ZynktXc414mnkEjEtjbCmkd4PD4clFZIwQvD9t/HudVVSKn0lVSnu7sHOPS9+3/ZsnV9+Q9L/f2DIEY0Ai2acemSYp1MWCqXV6hUyrNRW89d/Lnm8bLy/7dnMJn/zhlVgZmEf1g1iiC4RLqkrKBSYFG75HQiRD5fIBa/UlGHXZAmFx4FnQ6K/Pv4y204/x9XAIc+f94SpVIJlZ6fd/w478upB/adYrOp3VFFYMXIy9XJIFcWywwCwbathrYMeWVuUgHfpq5PvbSR0sq/n5RU+ra9tv4Dikolh8cAt1BrqTZds6bO4dsgIOFZvFwuV++Wi8rT01P9/ALBWXvXb/Ao7oHmI08eP0R/OWgN8fFxj18eh3AT4sjRH48vLS0pKipEFMdSyNTRpHDw83Zx8isuyba381C/bKxd6HQmj1dXl30Wk21t5ZSd81xz5FnSLaQzQIiOHq9tYdfaF2MuME9MTHiemACiiYgYXFlZsWLlYqg+JycnLln6Jdi87t2q14cfPPjDGzeuQvomJyf7/oM76zeubNSoqd+rQrx5K+bLr6Zdjr6QmZUBFzx8eJ+jg5ODgyOiOG4N+EUvdGVyOrT98NGTi1ALzstPy8xK2PP71xu3fVpRIa77U5Dlger2jTtHIZq8fO23rOxnSGeU5YttnV47J5HWXHP//pHLli/4fMqYRQu/b9G89fffbdyybf3YT4eBVQsOarx61WYrq+qW/i6de4BGQYhbt20AdbZt0+Gzz6b841IfDh8NcfSmTWsKCvPhnKCgRsuXrTOCPDBXwLCwZUlKKnhW2p8vr2Fgx2EDF128suvMhS1mZgIP94bjR/9oZvaG4QddO40VS0pOnF6nqlL5N2jTu9ukXfvnwjbSAeJCiU+/1yaDa5+E6daZIqjdN+pQV4SBM1F7sxq1s/QIxG4UyP1LxUnxKqGHFTIx5BWK4rSCIVNf2xeY9L7RK006WOclFqtUJjcLRUFKcUCLuobmkOGk+qZVb9vncUUOPra1lsbFR+87vKjWIj7XUiwtrf2aIf3Ce0xGWiIl7cHPu2tvQYAkUfVsq7WFSa2bD4Aseq2fqpTIK8oqgkLrivKJEPVN007WibGZCpmSya6llQUaRb6cdrTWD0LcDK1XtRYxGNqcmNTdNeh19wBt03Q6o9Z4vY57KM0qbT/AFtUJEaIB6D7C7sDqTJ+27v8ugnQgl2uODIp276EwrVToQK/f8A0XJDGiAbAUsjsOsUt/kI2MnZIckapS2mnIm1cnJkI0DN6NBJ0G26bdM2YtlmSJGErp4ClvNWqWCNFguHpz24RbJca8gHgRGR35yUUsJO0z9m2bIUiMaEjALtq7cs7szqtisOy8bIyj83ZprrgguahhG4vm3d/skTUQIRoYaGsB53UvqjjmRKqTrw3X0oxnSckxfmDXy/MlojyRpZAxcLKzld279VAhQsQCyOnA697F4vhbhRmlCmsX8ypEY3EYLDMGDdfV88B8y6QK0J9SoZKWSCtF8noB/NbDhY71/ksDJhEiRjTtaA0vcZkiPUFSlCMXlVTKpCqJCNPJzM1tWKwqlZWQYWXHdHAXOnm+19wVRIjYwbdg+jd/5xlXqc5rusuy6KoqCreHcs2ZyEQHbVKV2uMPviWjKLsSUZacFKmlkCzHRyVqF6KtI7uKsj1ElIoq+CFZESFSitqFKHThCKyYsdFFiIJE/54dFGr5urERBDypa73mqAP5dAatUZgNhIyICsgqVVcO5zRoIghoaXLBPtV5w8Lht88WxcWUghCrw3+M4QoYuWlScMfBbS19mhi49wrhP/AGIaKXSzCUFsglZbi3h0LtBMIJRKAmbxYigaAHiAkhYAERIgELiBAJWECESMACIkQCFhAhErDgfwAAAP//Icp0kQAAAAZJREFUAwBxq0dupf6LiwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e8e76-5d43-44cd-bf01-a39212cedd8d",
   "metadata": {},
   "source": [
    "We'll also define a utility to render the agent outputs nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16636975-5f2d-4dc7-ab8e-d0bea0830a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream, output_messages_key=\"llm_input_messages\"):\n",
    "    for chunk in stream:\n",
    "        for node, update in chunk.items():\n",
    "            print(f\"Update from node: {node}\")\n",
    "            messages_key = (\n",
    "                output_messages_key if node == \"pre_model_hook\" else \"messages\"\n",
    "            )\n",
    "            for message in update[messages_key]:\n",
    "                if isinstance(message, tuple):\n",
    "                    print(message)\n",
    "                else:\n",
    "                    message.pretty_print()\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84448d29-b323-4833-80fc-4fff2f5a0950",
   "metadata": {},
   "source": [
    "Now let's run the agent with a few different queries to reach the specified max tokens limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ffff6c3-a4f5-47c9-b51d-97caaee85cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\n",
    "result = graph.invoke(inputs, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb186da-b55d-4cb8-a237-e9e157ab0458",
   "metadata": {},
   "source": [
    "Let's see how many tokens we have in the message history so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ba0253-5199-4d29-82ae-258cbbebddb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = result[\"messages\"]\n",
    "count_tokens_approximately(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "812987ac-66ba-4122-8281-469cbdced7c7",
   "metadata": {},
   "source": [
    "You can see that we are close to the `max_tokens` threshold, so on the next invocation we should see `pre_model_hook` kick-in and trim the message history. Let's run it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c53429-90ba-4d0b-abb9-423d9120ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: pre_model_hook\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's it known for?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "New York City is known for a variety of iconic landmarks, cultural institutions, and vibrant neighborhoods. Some of the most notable features include:\n",
      "\n",
      "1. **Statue of Liberty**: A symbol of freedom and democracy, located on Liberty Island.\n",
      "2. **Times Square**: Known for its bright lights, Broadway theaters, and bustling atmosphere.\n",
      "3. **Central Park**: A large public park offering a natural retreat in the midst of the city.\n",
      "4. **Empire State Building**: An iconic skyscraper offering panoramic views of the city.\n",
      "5. **Broadway**: Famous for its world-class theater productions.\n",
      "6. **Wall Street**: The financial hub of the United States.\n",
      "7. **Museums**: Including the Metropolitan Museum of Art, Museum of Modern Art (MoMA), and the American Museum of Natural History.\n",
      "8. **Diverse Cuisine**: A melting pot of cultures reflected in its diverse food scene.\n",
      "9. **Cultural Diversity**: A rich tapestry of cultures and ethnicities, contributing to its dynamic atmosphere.\n",
      "10. **Fashion**: A global fashion capital, hosting events like New York Fashion Week.\n",
      "\n",
      "These are just a few highlights of what makes New York City a unique and exciting place.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "where can i find the best bagel?\n",
      "\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Finding the \"best\" bagel in New York City can be subjective, as it often depends on personal taste. However, several bagel shops are frequently mentioned as top contenders:\n",
      "\n",
      "1. **Ess-a-Bagel**: Known for its large, chewy bagels and a wide variety of spreads.\n",
      "2. **Russ & Daughters**: Famous for its bagels with lox and other traditional Jewish delicacies.\n",
      "3. **H&H Bagels**: A classic choice, known for its fresh, hand-rolled bagels.\n",
      "4. **Murray’s Bagels**: Offers a wide selection of bagels and toppings, with a focus on traditional methods.\n",
      "5. **Tompkins Square Bagels**: Known for its creative cream cheese flavors and fresh ingredients.\n",
      "6. **Absolute Bagels**: A favorite on the Upper West Side, known for its authentic taste and texture.\n",
      "7. **Bagel Hole**: A small shop in Brooklyn known for its dense, flavorful bagels.\n",
      "\n",
      "These spots are scattered throughout the city, so you can find a great bagel in various neighborhoods. Each of these places has its own unique style and flavor, so it might be worth trying a few to find your personal favorite!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\n",
    "print_stream(graph.stream(inputs, config=config, stream_mode=\"updates\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe0399-4e7d-4482-a4cb-5301311932d0",
   "metadata": {},
   "source": [
    "You can see that the `pre_model_hook` node now only returned the last 3 messages, as expected. However, the existing message history is untouched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ecfc310-8f9e-4aa0-9e58-17e71551639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_messages = graph.get_state(config).values[\"messages\"]\n",
    "assert [(m.type, m.content) for m in updated_messages[: len(messages)]] == [\n",
    "    (m.type, m.content) for m in messages\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035864e3-0083-4dea-bf85-3a702fa5303f",
   "metadata": {},
   "source": [
    "## Overwrite the original message history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a4fd5-a2ba-4eca-91a9-d294f4f2d884",
   "metadata": {},
   "source": [
    "Let's now change the `pre_model_hook` to **overwrite** the message history in the graph state. To do this, we’ll return the updated messages under `messages` key. We’ll also include a special `RemoveMessage(REMOVE_ALL_MESSAGES)` object, which tells `create_react_agent` to remove previous messages from the graph state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c2a65b-685a-4750-baa6-2d61efe76b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m20:30:48\u001b[0m \u001b[34mredisvl.index.index\u001b[0m \u001b[1;30mINFO\u001b[0m   Index already exists, not overwriting.\n",
      "\u001b[32m20:30:48\u001b[0m \u001b[34mredisvl.index.index\u001b[0m \u001b[1;30mINFO\u001b[0m   Index already exists, not overwriting.\n",
      "\u001b[32m20:30:48\u001b[0m \u001b[34mredisvl.index.index\u001b[0m \u001b[1;30mINFO\u001b[0m   Index already exists, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "\n",
    "def pre_model_hook(state):\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=384,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    # NOTE that we're now returning the messages under the `messages` key\n",
    "    # We also remove the existing messages in the history to ensure we're overwriting the history\n",
    "    # highlight-next-line\n",
    "    return {\"messages\": [RemoveMessage(REMOVE_ALL_MESSAGES)] + trimmed_messages}\n",
    "\n",
    "\n",
    "# Set up Redis connection for checkpointer\n",
    "REDIS_URI = \"redis://redis:6379\"\n",
    "checkpointer = None\n",
    "with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "    cp.setup()\n",
    "    checkpointer = cp\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    # highlight-next-line\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd061682-231c-4487-9c2f-a6820dfbcab7",
   "metadata": {},
   "source": [
    "Now let's run the agent with the same queries as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831be36a-78a1-4885-9a03-8d085dfd7e37",
   "metadata": {},
   "outputs": [
    {
     "ename": "RedisSearchError",
     "evalue": "Error while searching: checkpoints_blobs: no such index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redisvl/index/index.py:795\u001b[39m, in \u001b[36mSearchIndex.search\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_redis_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redis/commands/search/commands.py:508\u001b[39m, in \u001b[36mSearchCommands.search\u001b[39m\u001b[34m(self, query, query_params)\u001b[39m\n\u001b[32m    506\u001b[39m     options[NEVER_DECODE] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSEARCH_CMD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, Pipeline):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redis/client.py:559\u001b[39m, in \u001b[36mRedis.execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **options):\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redis/client.py:567\u001b[39m, in \u001b[36mRedis._execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_disconnect_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redis/retry.py:62\u001b[39m, in \u001b[36mRetry.call_with_retry\u001b[39m\u001b[34m(self, do, fail)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m._supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redis/client.py:568\u001b[39m, in \u001b[36mRedis._execute_command.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    567\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m conn.retry.call_with_retry(\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    571\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m._disconnect_raise(conn, error),\n\u001b[32m    572\u001b[39m     )\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redis/client.py:542\u001b[39m, in \u001b[36mRedis._send_command_parse_response\u001b[39m\u001b[34m(self, conn, command_name, *args, **options)\u001b[39m\n\u001b[32m    541\u001b[39m conn.send_command(*args, **options)\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redis/client.py:581\u001b[39m, in \u001b[36mRedis.parse_response\u001b[39m\u001b[34m(self, connection, command_name, **options)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m NEVER_DECODE \u001b[38;5;129;01min\u001b[39;00m options:\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisable_decoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m     options.pop(NEVER_DECODE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redis/connection.py:616\u001b[39m, in \u001b[36mAbstractConnection.read_response\u001b[39m\u001b[34m(self, disable_decoding, disconnect_on_error, push_request)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m response\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mResponseError\u001b[39m: checkpoints_blobs: no such index",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRedisSearchError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m messages = result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [(\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhere can i find the best bagel?\u001b[39m\u001b[33m\"\u001b[39m)]}\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mprint_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_messages_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mprint_stream\u001b[39m\u001b[34m(stream, output_messages_key)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_stream\u001b[39m(stream, output_messages_key=\u001b[33m\"\u001b[39m\u001b[33mllm_input_messages\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUpdate from node: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2377\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_during \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2376\u001b[39m     config[CONF][CONFIG_KEY_CHECKPOINT_DURING] = checkpoint_during\n\u001b[32m-> \u001b[39m\u001b[32m2377\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSyncPregelLoop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStreamProtocol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_modes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2386\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_channels_asis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2388\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2389\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\n\u001b[32m   2393\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2394\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCONF\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG_KEY_CHECKPOINT_DURING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmigrate_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_migrate_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2397\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2398\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# create runner\u001b[39;49;00m\n\u001b[32m   2399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPregelRunner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCONF\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_RUNNER_SUBMIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWeakMethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnode_finished\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCONF\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG_KEY_NODE_FINISHED\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2406\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2407\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# enable subgraph streaming\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/langgraph/pregel/loop.py:1058\u001b[39m, in \u001b[36mSyncPregelLoop.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CheckpointNotLatest\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checkpointer:\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m     saved = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1060\u001b[39m     saved = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/checkpoint-redis/langgraph/checkpoint/redis/__init__.py:335\u001b[39m, in \u001b[36mRedisSaver.get_tuple\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    332\u001b[39m doc_parent_checkpoint_id = from_storage_safe_id(doc[\u001b[33m\"\u001b[39m\u001b[33mparent_checkpoint_id\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# Fetch channel_values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m channel_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_channel_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoc_thread_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_ns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoc_checkpoint_ns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoc_checkpoint_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Fetch pending_sends from parent checkpoint\u001b[39;00m\n\u001b[32m    342\u001b[39m pending_sends = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/checkpoint-redis/langgraph/checkpoint/redis/__init__.py:452\u001b[39m, in \u001b[36mRedisSaver.get_channel_values\u001b[39m\u001b[34m(self, thread_id, checkpoint_ns, checkpoint_id)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m channel, version \u001b[38;5;129;01min\u001b[39;00m channel_versions.items():\n\u001b[32m    443\u001b[39m     blob_query = FilterQuery(\n\u001b[32m    444\u001b[39m         filter_expression=(Tag(\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m) == storage_safe_thread_id)\n\u001b[32m    445\u001b[39m         & (Tag(\u001b[33m\"\u001b[39m\u001b[33mcheckpoint_ns\u001b[39m\u001b[33m\"\u001b[39m) == storage_safe_checkpoint_ns)\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m         num_results=\u001b[32m1\u001b[39m,\n\u001b[32m    450\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     blob_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint_blobs_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m blob_results.docs:\n\u001b[32m    454\u001b[39m         blob_doc = blob_results.docs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/redisvl/index/index.py:799\u001b[39m, in \u001b[36mSearchIndex.search\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._redis_client.ft(\u001b[38;5;28mself\u001b[39m.schema.index.name).search(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    796\u001b[39m         *args, **kwargs\n\u001b[32m    797\u001b[39m     )\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RedisSearchError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while searching: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRedisSearchError\u001b[39m: Error while searching: checkpoints_blobs: no such index"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "messages = result[\"messages\"]\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\n",
    "print_stream(\n",
    "    graph.stream(inputs, config=config, stream_mode=\"updates\"),\n",
    "    output_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a0604-3d2b-48ff-9eaf-d16ea351fb30",
   "metadata": {},
   "source": [
    "You can see that the `pre_model_hook` node returned the last 3 messages again. However, this time, the message history is modified in the graph state as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f72f8-f817-472d-a193-e01509a86132",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_messages = graph.get_state(config).values[\"messages\"]\n",
    "assert (\n",
    "    # First 2 messages in the new history are the same as last 2 messages in the old\n",
    "    [(m.type, m.content) for m in updated_messages[:2]]\n",
    "    == [(m.type, m.content) for m in messages[-2:]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee186d6d-4d07-404f-b236-f662db62339d",
   "metadata": {},
   "source": [
    "## Summarizing message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langmem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e53e0f-9a1e-4188-8435-c23ad8148b4f",
   "metadata": {},
   "source": [
    "Finally, let's apply a different strategy for managing message history — summarization. Just as with trimming, you can choose to keep original message history unmodified or overwrite it. The example below will only show the former.\n",
    "\n",
    "We will use the [`SummarizationNode`](https://langchain-ai.github.io/langmem/guides/summarization/#using-summarizationnode) from the prebuilt `langmem` library. Once the message history reaches the token limit, the summarization node will summarize earlier messages to make sure they fit into `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9540c1c-2eba-42da-ba4e-478521161a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight-next-line\n",
    "from langmem.short_term import SummarizationNode\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "from typing import Any\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "summarization_node = SummarizationNode(\n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=summarization_model,\n",
    "    max_tokens=384,\n",
    "    max_summary_tokens=128,\n",
    "    output_messages_key=\"llm_input_messages\",\n",
    ")\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    # NOTE: we're adding this key to keep track of previous summary information\n",
    "    # to make sure we're not summarizing on every LLM call\n",
    "    # highlight-next-line\n",
    "    context: dict[str, Any]\n",
    "\n",
    "\n",
    "# Set up Redis connection for checkpointer\n",
    "REDIS_URI = \"redis://redis:6379\"\n",
    "checkpointer = None\n",
    "with RedisSaver.from_conn_string(REDIS_URI) as cp:\n",
    "    cp.setup()\n",
    "    checkpointer = cp\n",
    "\n",
    "graph = create_react_agent(\n",
    "    # limit the output size to ensure consistent behavior\n",
    "    model.bind(max_tokens=256),\n",
    "    tools,\n",
    "    # highlight-next-line\n",
    "    pre_model_hook=summarization_node,\n",
    "    # highlight-next-line\n",
    "    state_schema=State,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccaaca-5d9c-4faf-b997-d4b8e84b59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"What's it known for?\")]}\n",
    "result = graph.invoke(inputs, config=config)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"where can i find the best bagel?\")]}\n",
    "print_stream(graph.stream(inputs, config=config, stream_mode=\"updates\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caaf2f7-281a-4421-bf98-c745d950c56f",
   "metadata": {},
   "source": [
    "You can see that the earlier messages have now been replaced with the summary of the earlier conversation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
